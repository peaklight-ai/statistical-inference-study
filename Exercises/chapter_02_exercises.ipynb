{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Properties of Estimators - Exercises\n",
    "\n",
    "<div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 30px; border-radius: 10px; color: white;'>\n",
    "<h1 style='margin: 0; font-size: 2.5em;'>PLAI Academy</h1>\n",
    "<p style='margin: 10px 0 0 0; font-size: 1.2em; opacity: 0.9;'>Statistical Inference • Chapter 2 Exercises</p>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise Structure\n",
    "\n",
    "**20 exercises** covering bias, variance, efficiency, consistency, and MSE:\n",
    "\n",
    "1. **Exercises 1-5**: Core estimator properties\n",
    "2. **Exercises 6-10**: Advanced topics from statistics textbooks\n",
    "3. **Exercises 11-15**: Machine learning applications\n",
    "4. **Exercises 16-20**: Current research (2025+)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import trim_mean\n",
    "sns.set_theme(style='whitegrid', palette='husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Core Concepts (Exercises 1-5)\n",
    "\n",
    "### Exercise 2.1: Verifying Unbiasedness\n",
    "\n",
    "**Tasks:**\n",
    "1. Create a population N(μ=50, σ²=100)\n",
    "2. Draw 10,000 samples of size n=30\n",
    "3. For each sample, calculate:\n",
    "   - Sample mean X̄\n",
    "   - Biased variance estimator: σ̂² = (1/n)Σ(xᵢ-X̄)²\n",
    "   - Unbiased variance estimator: s² = (1/(n-1))Σ(xᵢ-X̄)²\n",
    "4. Calculate E[X̄], E[σ̂²], and E[s²]\n",
    "5. Verify: E[X̄] = μ, E[s²] = σ², E[σ̂²] < σ²\n",
    "6. Calculate the bias of σ̂²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Bias-Variance Tradeoff\n",
    "\n",
    "**Reference**: [Stanford CS109 - Properties of Estimators](https://web.stanford.edu/class/archive/cs/cs109/cs109.1218/files/student_drive/7.6.pdf)\n",
    "\n",
    "**Tasks:**\n",
    "1. Create a population with true parameter θ = 10\n",
    "2. Compare three estimators:\n",
    "   - Unbiased: θ̂₁ = X̄ (sample mean)\n",
    "   - Biased but lower variance: θ̂₂ = 0.9X̄ + 1\n",
    "   - Highly biased: θ̂₃ = 8 (constant)\n",
    "3. For each estimator, calculate:\n",
    "   - Bias\n",
    "   - Variance\n",
    "   - MSE = Bias² + Variance\n",
    "4. Which estimator has lowest MSE?\n",
    "5. Plot MSE decomposition (bias² vs variance) for each estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3: Efficiency Comparison\n",
    "\n",
    "**Tasks:**\n",
    "1. For a normal population N(100, 15²), compare three location estimators:\n",
    "   - Sample mean\n",
    "   - Sample median\n",
    "   - 10% trimmed mean\n",
    "2. Generate 5000 samples of size n=50 for each\n",
    "3. Calculate variance of each estimator\n",
    "4. Compute relative efficiency: Eff(median, mean) = Var(median)/Var(mean)\n",
    "5. Verify the theoretical result: median has ~64% efficiency for normal data\n",
    "6. Repeat for exponential population - which estimator is most efficient now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.4: Consistency Demonstration\n",
    "\n",
    "**Tasks:**\n",
    "1. Consider estimating the population mean from Uniform(0, θ)\n",
    "2. One estimator: θ̂₁ = 2X̄ (method of moments)\n",
    "3. Another: θ̂₂ = max(X₁,...,Xₙ) × (n+1)/n (maximum likelihood adjusted)\n",
    "4. For n = [10, 30, 100, 500, 2000]:\n",
    "   - Generate 1000 samples of each size\n",
    "   - Calculate both estimators\n",
    "   - Plot distributions\n",
    "5. Show both estimators are consistent by examining how distributions concentrate\n",
    "6. Which converges faster? Calculate MSE(n) for both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.5: Cramér-Rao Lower Bound\n",
    "\n",
    "**Reference**: [Bias, MSE, Relative Efficiency](https://uregina.ca/~kozdron/Teaching/Regina/252Winter16/Handouts/ch3.pdf)\n",
    "\n",
    "**Tasks:**\n",
    "1. For Poisson(λ), the Fisher Information is I(λ) = 1/λ\n",
    "2. Generate data from Poisson(λ=5) with n=30\n",
    "3. The sample mean X̄ estimates λ\n",
    "4. Calculate:\n",
    "   - Cramér-Rao Lower Bound: CRLB = 1/(n×I(λ))\n",
    "   - Empirical variance of X̄ from 10,000 samples\n",
    "   - Theoretical variance: Var(X̄) = λ/n\n",
    "5. Show that X̄ achieves the CRLB (it's efficient)\n",
    "6. Try a different estimator (e.g., median) and show it has higher variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Advanced Problems (Exercises 6-10)\n",
    "\n",
    "### Exercise 2.6: Rao-Blackwell Improvement\n",
    "\n",
    "**Tasks:**\n",
    "1. For Bernoulli(p) data with n observations\n",
    "2. Start with crude estimator: θ̂₀ = X₁ (just the first observation)\n",
    "3. The sufficient statistic is T = ΣXᵢ\n",
    "4. Apply Rao-Blackwell: θ̂* = E[θ̂₀|T] = T/n (the sample proportion)\n",
    "5. Simulate p=0.6, n=20 with 5000 repetitions\n",
    "6. Calculate Var(θ̂₀) and Var(θ̂*)\n",
    "7. Verify: Var(θ̂*) = Var(θ̂₀) × (1/n)\n",
    "8. Show both are unbiased but θ̂* has much lower variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.7: James-Stein Estimator\n",
    "\n",
    "The famous James-Stein estimator shows that X̄ is inadmissible for dimensions ≥3.\n",
    "\n",
    "**Tasks:**\n",
    "1. Generate p=5 means μ = [1, 2, 3, 4, 5]\n",
    "2. Observe X ~ N(μ, I) where I is identity matrix\n",
    "3. Compare estimators:\n",
    "   - MLE: δ̂_MLE = X\n",
    "   - James-Stein: δ̂_JS = (1 - (p-2)/(||X||²)) × X\n",
    "4. Repeat 10,000 times and calculate:\n",
    "   - Average squared error: ||δ̂ - μ||²\n",
    "5. Show James-Stein dominates MLE (lower MSE)\n",
    "6. This violates intuition that X̄ is optimal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.8: Robustness vs Efficiency\n",
    "\n",
    "**Tasks:**\n",
    "1. Generate clean data: N(50, 10²), n=40\n",
    "2. Generate contaminated data: 90% N(50, 10²) + 10% N(50, 50²)\n",
    "3. Compare estimators: mean, median, 10% trimmed mean, 20% trimmed mean\n",
    "4. For clean data, calculate:\n",
    "   - MSE of each estimator\n",
    "   - Relative efficiency vs mean\n",
    "5. For contaminated data, recalculate MSE\n",
    "6. Create a table showing MSE(clean) and MSE(contaminated) for each\n",
    "7. Which estimator offers best efficiency-robustness balance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.9: Asymptotic Normality\n",
    "\n",
    "**Tasks:**\n",
    "1. For exponential data Exp(λ=2), the MLE is λ̂ = 1/X̄\n",
    "2. For sample sizes n = [10, 30, 100, 500]:\n",
    "   - Generate 5000 samples\n",
    "   - Calculate λ̂ for each\n",
    "   - Standardize: Z = √n(λ̂ - λ) / se(λ̂)\n",
    "3. Plot standardized distributions\n",
    "4. Overlay N(0,1) density\n",
    "5. Use Kolmogorov-Smirnov test to measure convergence to normality\n",
    "6. At what sample size is normal approximation accurate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.10: Bootstrap Standard Errors\n",
    "\n",
    "**Reference**: [Estimation Theory](http://www.dliebl.com/RM_ES_Script/estimation-theory.html)\n",
    "\n",
    "**Tasks:**\n",
    "1. Generate original sample of n=50 from Gamma(shape=3, scale=2)\n",
    "2. Calculate these statistics: mean, median, 20% trimmed mean, standard deviation\n",
    "3. For each statistic:\n",
    "   - Perform 5000 bootstrap resamples\n",
    "   - Calculate the statistic on each resample\n",
    "   - Estimate SE from bootstrap distribution\n",
    "4. Compare bootstrap SE to theoretical SE (where known)\n",
    "5. Construct 95% bootstrap confidence intervals using:\n",
    "   - Percentile method\n",
    "   - Normal approximation method\n",
    "6. Which method gives more accurate coverage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: ML Applications (Exercises 11-15)\n",
    "\n",
    "### Exercise 2.11: Bias-Variance in Ridge Regression\n",
    "\n",
    "Ridge regression introduces bias to reduce variance.\n",
    "\n",
    "**Tasks:**\n",
    "1. Generate data: y = Xβ + ε with p=10 features, n=50, high correlation between features\n",
    "2. For λ = [0, 0.1, 1, 10, 100]:\n",
    "   - Generate 200 training sets\n",
    "   - Fit ridge regression: β̂ = (X'X + λI)⁻¹X'y\n",
    "   - Predict on fixed test point\n",
    "3. For each λ, calculate:\n",
    "   - Bias² of predictions\n",
    "   - Variance of predictions\n",
    "   - Total MSE\n",
    "4. Plot bias², variance, and MSE vs λ\n",
    "5. Find optimal λ that minimizes MSE\n",
    "6. Compare to OLS (λ=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.12: Consistency of SGD Estimators\n",
    "\n",
    "Stochastic Gradient Descent produces estimators that should be consistent.\n",
    "\n",
    "**Tasks:**\n",
    "1. True model: y = 3x + 2 + ε\n",
    "2. For dataset sizes n = [100, 500, 1000, 5000, 10000]:\n",
    "   - Generate data\n",
    "   - Run mini-batch SGD with batch_size=32\n",
    "   - Record final β̂ estimates\n",
    "3. Repeat 100 times for each n\n",
    "4. Plot distribution of β̂ for each sample size\n",
    "5. Calculate:\n",
    "   - Bias(β̂) vs n\n",
    "   - Var(β̂) vs n\n",
    "   - MSE(β̂) vs n\n",
    "6. Verify: MSE → 0 as n → ∞ (consistency)\n",
    "7. How does learning rate affect consistency?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.13: Estimator Comparison for Model Selection\n",
    "\n",
    "**Tasks:**\n",
    "1. Generate data with 20 features, only 5 truly predictive\n",
    "2. Compare model selection criteria:\n",
    "   - AIC = 2k - 2ln(L)\n",
    "   - BIC = k×ln(n) - 2ln(L)\n",
    "   - Cross-validation MSE\n",
    "3. For each criterion:\n",
    "   - Try all possible feature subsets (or use forward selection)\n",
    "   - Select best model\n",
    "4. Repeat 200 times with different data\n",
    "5. Calculate for each criterion:\n",
    "   - Probability of selecting correct features\n",
    "   - Average number of false positives\n",
    "   - Average test MSE\n",
    "6. Which criterion is most consistent at finding the true model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.14: Variance Estimation in Deep Learning\n",
    "\n",
    "**Tasks:**\n",
    "1. Train a neural network on a regression task\n",
    "2. Estimate prediction variance using three methods:\n",
    "   - Monte Carlo dropout (run forward pass 100 times with dropout enabled)\n",
    "   - Bootstrap (retrain on 50 bootstrapped datasets)\n",
    "   - Ensemble (train 50 networks with different initializations)\n",
    "3. For a test set:\n",
    "   - Get mean prediction and variance from each method\n",
    "   - Compare variance estimates\n",
    "4. Evaluate uncertainty quality:\n",
    "   - Do wider intervals have higher error?\n",
    "   - Calculate calibration\n",
    "5. Which method gives most reliable uncertainty estimates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.15: Efficient Estimation in Logistic Regression\n",
    "\n",
    "**Tasks:**\n",
    "1. Generate binary classification data with known β\n",
    "2. Compare three estimators for β:\n",
    "   - Maximum Likelihood (standard logistic regression)\n",
    "   - Median-unbiased estimator (using Firth's correction)\n",
    "   - Bayesian posterior mean with weak prior\n",
    "3. For n = [50, 100, 500]:\n",
    "   - Generate 500 datasets\n",
    "   - Fit all three methods\n",
    "4. Calculate for each method:\n",
    "   - Bias\n",
    "   - Variance\n",
    "   - MSE\n",
    "5. Which achieves lowest MSE?\n",
    "6. How does sample size affect relative performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Contemporary Research (2025+)\n",
    "\n",
    "### Exercise 2.16: Estimator Properties in Double Descent\n",
    "\n",
    "**Reference**: [Rethinking Bias-Variance for Neural Networks (2025)](https://dl.acm.org/doi/pdf/10.5555/3524938.3525936)\n",
    "\n",
    "Modern neural networks exhibit \"double descent\" where increasing parameters past interpolation reduces error.\n",
    "\n",
    "**Tasks:**\n",
    "1. Generate data: y = f(x) + ε with n=100 samples\n",
    "2. Fit polynomial models of degree d = [2, 5, 10, 20, 50, 99, 100, 150, 200]\n",
    "3. For each degree (repeat 200 times):\n",
    "   - Train on all n samples\n",
    "   - Predict on test set\n",
    "4. Calculate:\n",
    "   - Bias²(d)\n",
    "   - Variance(d)\n",
    "   - Test MSE(d)\n",
    "5. Plot all three vs model complexity\n",
    "6. Observe:\n",
    "   - Classical U-shape (underfitting → overfitting)\n",
    "   - Second descent after interpolation threshold\n",
    "7. Does variance continue increasing, or does it decrease in over-parameterized regime?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.17: Uncertainty in LLM Reward Models\n",
    "\n",
    "**Reference**: [Uncertainty Quantification for LLM Reward Learning (arxiv 2025)](https://arxiv.org/abs/2512.03208)\n",
    "\n",
    "**Tasks:**\n",
    "1. Simulate reward model: given pairs (response_A, response_B), predict which is better\n",
    "2. True preferences follow Bradley-Terry model: P(A > B) = exp(r_A)/(exp(r_A)+exp(r_B))\n",
    "3. Collect n comparisons, fit reward model\n",
    "4. For the reward estimate r̂:\n",
    "   - Calculate asymptotic variance using Fisher Information\n",
    "   - Construct confidence intervals\n",
    "5. Verify coverage by simulation:\n",
    "   - Generate 1000 datasets\n",
    "   - For each, construct 95% CI\n",
    "   - Check if true r is contained\n",
    "6. How does uncertainty scale with n?\n",
    "7. What if there are heterogeneous annotators (some more reliable)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.18: Efficiency of LLM Survey Estimators\n",
    "\n",
    "**Reference**: [How Many Survey Respondents is an LLM Worth? (arxiv 2025)](https://arxiv.org/abs/2502.17773)\n",
    "\n",
    "**Tasks:**\n",
    "1. True population has opinion distribution: [40% A, 35% B, 25% C]\n",
    "2. LLM simulates responses with slight bias: [42% A, 34% B, 24% C]\n",
    "3. Compare two estimators:\n",
    "   - Real survey: p̂_real from n_real human responses\n",
    "   - LLM-augmented: combine n_real humans + n_LLM simulated responses\n",
    "4. For fixed budget (n_real = 100):\n",
    "   - Vary n_LLM = [0, 100, 500, 1000, 5000]\n",
    "   - Calculate MSE of combined estimator\n",
    "5. Find optimal weight: p̂ = w×p̂_real + (1-w)×p̂_LLM\n",
    "6. When does adding LLM data help vs hurt?\n",
    "7. Propose efficiency adjustment for LLM bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.19: Causal Inference Estimator Comparison\n",
    "\n",
    "**Reference**: [A/B Testing at Scale (ResearchGate 2025)](https://www.researchgate.net/publication/392470551)\n",
    "\n",
    "**Tasks:**\n",
    "1. Simulate A/B test with n=100,000 users\n",
    "2. Treatment effect: τ = 0.03 (3% lift)\n",
    "3. Add confounders (user characteristics that affect both treatment and outcome)\n",
    "4. Compare estimators:\n",
    "   - Simple difference in means (ignoring confounders)\n",
    "   - Regression adjustment\n",
    "   - Inverse propensity weighting\n",
    "   - Doubly-robust estimator\n",
    "5. For each, calculate:\n",
    "   - Bias\n",
    "   - Variance\n",
    "   - MSE\n",
    "   - Coverage of 95% CI\n",
    "6. Which estimator is most efficient?\n",
    "7. How do results change with perfect randomization vs observational data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.20: Bootstrap Bias Correction in Model Evaluation\n",
    "\n",
    "**Reference**: [Bootstrap Bias Corrected Cross Validation (PMC 2025)](https://pmc.ncbi.nlm.nih.gov/articles/PMC7304018/)\n",
    "\n",
    "**Tasks:**\n",
    "1. Train a complex model (e.g., random forest) on n=200 samples\n",
    "2. Evaluate performance using:\n",
    "   - Training error (optimistic bias)\n",
    "   - Standard cross-validation\n",
    "   - Bootstrap .632 estimator: 0.632×E_test + 0.368×E_train\n",
    "   - Bootstrap .632+ (bias-corrected version)\n",
    "3. Generate 100 independent test sets to get \"true\" performance\n",
    "4. For each evaluation method, calculate:\n",
    "   - Bias (compared to true performance)\n",
    "   - Variance across 50 repetitions\n",
    "   - MSE\n",
    "5. Which method gives most accurate performance estimate?\n",
    "6. How does model complexity affect the bias?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise Completion Checklist\n",
    "\n",
    "- [ ] Exercises 1-5: Core properties (bias, variance, efficiency)\n",
    "- [ ] Exercises 6-10: Advanced topics (Rao-Blackwell, James-Stein, robustness)\n",
    "- [ ] Exercises 11-15: ML applications (Ridge, SGD, neural networks)\n",
    "- [ ] Exercises 16-20: 2025+ research (double descent, LLMs, causal inference)\n",
    "\n",
    "---\n",
    "\n",
    "<div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 20px; border-radius: 10px; color: white; text-align: center;'>\n",
    "<p style='margin: 0; font-size: 1.1em;'>Exercises curated by <strong>PLAI Academy</strong></p>\n",
    "<p style='margin: 5px 0 0 0; opacity: 0.8;'>Statistical Inference • 2025</p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
