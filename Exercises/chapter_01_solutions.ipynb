{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Introduction to Statistical Inference - Solutions\n",
    "\n",
    "<div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 30px; border-radius: 10px; color: white;'>\n",
    "<h1 style='margin: 0; font-size: 2.5em;'>PLAI Academy</h1>\n",
    "<p style='margin: 10px 0 0 0; font-size: 1.2em; opacity: 0.9;'>Statistical Inference • Chapter 1 Solutions</p>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "sns.set_theme(style='whitegrid', palette='husl')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Fundamental Concepts\n",
    "\n",
    "### Solution 1.1: Population vs Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Population: All 10,000 boxes produced today. Sample: 30 randomly selected boxes.\n",
    "\n",
    "# 2. Generate population\n",
    "population = np.random.normal(loc=500, scale=15, size=10000)\n",
    "true_mu = population.mean()\n",
    "print(f\"True population mean: {true_mu:.2f}g\")\n",
    "\n",
    "# 3. Draw random sample\n",
    "sample = np.random.choice(population, size=30, replace=False)\n",
    "\n",
    "# 4. Calculate sample mean and compare\n",
    "sample_mean = sample.mean()\n",
    "print(f\"Sample mean: {sample_mean:.2f}g\")\n",
    "print(f\"Difference: {abs(sample_mean - true_mu):.2f}g\")\n",
    "\n",
    "# 5. Repeat 10 times\n",
    "sample_means = [np.random.choice(population, size=30, replace=False).mean() for _ in range(10)]\n",
    "print(f\"\\n10 sample means: {[f'{m:.2f}' for m in sample_means]}\")\n",
    "print(f\"Average of sample means: {np.mean(sample_means):.2f}g\")\n",
    "print(f\"Std dev of sample means: {np.std(sample_means):.2f}g\")\n",
    "\n",
    "# Observation: Sample means vary around true mean (unbiasedness)\n",
    "# Standard deviation ≈ σ/√n = 15/√30 ≈ 2.74"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1.2: Sampling Distribution Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-2. Create population and draw 1000 samples\n",
    "population = stats.norm(loc=100, scale=15)  # variance = 225\n",
    "sample_means = [population.rvs(25).mean() for _ in range(1000)]\n",
    "\n",
    "# 3. Calculate sample mean for each (done above)\n",
    "\n",
    "# 4. Plot distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(sample_means, bins=40, density=True, alpha=0.7, edgecolor='black', label='Empirical')\n",
    "\n",
    "# Overlay theoretical distribution: N(100, 225/25) = N(100, 9)\n",
    "x = np.linspace(90, 110, 100)\n",
    "plt.plot(x, stats.norm(100, 3).pdf(x), 'r-', linewidth=2, label='Theoretical N(100, 3²)')\n",
    "\n",
    "plt.xlabel('Sample Mean')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Sampling Distribution of X̄ (n=25)')\n",
    "plt.legend()\n",
    "plt.axvline(100, color='green', linestyle='--', linewidth=2, label='True μ')\n",
    "plt.show()\n",
    "\n",
    "# 5. Compare to theoretical values\n",
    "print(f\"Empirical mean: {np.mean(sample_means):.3f} (theoretical: 100)\")\n",
    "print(f\"Empirical SE: {np.std(sample_means):.3f} (theoretical: 3.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1.3: Central Limit Theorem with Non-Normal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create exponential population\n",
    "population = stats.expon(scale=2)  # mean=2, variance=4\n",
    "\n",
    "# 2. For each sample size\n",
    "sample_sizes = [5, 10, 30, 50, 100]\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, n in enumerate(sample_sizes):\n",
    "    # Draw 1000 samples and calculate means\n",
    "    sample_means = [population.rvs(n).mean() for _ in range(1000)]\n",
    "    \n",
    "    # Plot distribution\n",
    "    axes[idx].hist(sample_means, bins=30, density=True, alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    # Overlay theoretical normal\n",
    "    x = np.linspace(min(sample_means), max(sample_means), 100)\n",
    "    theoretical_std = 2 / np.sqrt(n)  # σ/√n\n",
    "    axes[idx].plot(x, stats.norm(2, theoretical_std).pdf(x), 'r-', linewidth=2)\n",
    "    \n",
    "    axes[idx].set_title(f'n = {n}')\n",
    "    axes[idx].axvline(2, color='green', linestyle='--')\n",
    "\n",
    "axes[-1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Calculate skewness (measures departure from normality)\n",
    "from scipy.stats import skew\n",
    "print(\"\\nSkewness by sample size:\")\n",
    "for n in sample_sizes:\n",
    "    sample_means = [population.rvs(n).mean() for _ in range(1000)]\n",
    "    print(f\"n={n:3d}: skewness = {skew(sample_means):.3f}\")\n",
    "print(\"\\nSkewness → 0 as n increases (approaching normality)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1.4: Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True parameters\n",
    "true_mu = 75\n",
    "true_sigma = 12\n",
    "n = 50\n",
    "alpha = 0.05\n",
    "\n",
    "# 1-2. Generate 100 confidence intervals\n",
    "intervals = []\n",
    "contains_mu = []\n",
    "\n",
    "for _ in range(100):\n",
    "    sample = np.random.normal(true_mu, true_sigma, n)\n",
    "    sample_mean = sample.mean()\n",
    "    sample_std = sample.std(ddof=1)\n",
    "    \n",
    "    # t critical value\n",
    "    t_crit = stats.t.ppf(1 - alpha/2, df=n-1)\n",
    "    \n",
    "    # Confidence interval\n",
    "    margin = t_crit * (sample_std / np.sqrt(n))\n",
    "    ci_lower = sample_mean - margin\n",
    "    ci_upper = sample_mean + margin\n",
    "    \n",
    "    intervals.append((ci_lower, ci_upper))\n",
    "    contains_mu.append(ci_lower <= true_mu <= ci_upper)\n",
    "\n",
    "# 4. Plot all intervals\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, (ci_lower, ci_upper) in enumerate(intervals):\n",
    "    color = 'green' if contains_mu[i] else 'red'\n",
    "    plt.plot([ci_lower, ci_upper], [i, i], color=color, linewidth=1)\n",
    "\n",
    "plt.axvline(true_mu, color='blue', linestyle='--', linewidth=2, label=f'True μ = {true_mu}')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('CI Number')\n",
    "plt.title('100 Confidence Intervals (95% level)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 5. Calculate coverage rate\n",
    "coverage = np.mean(contains_mu)\n",
    "print(f\"Coverage rate: {coverage:.1%} (expected: 95%)\")\n",
    "print(f\"Number containing μ: {sum(contains_mu)}/100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1.5: Hypothesis Testing and P-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given data\n",
    "mu_0 = 70  # null hypothesis mean\n",
    "x_bar = 73.5\n",
    "s = 8\n",
    "n = 36\n",
    "\n",
    "# 1. Hypotheses: H₀: μ = 70, H₁: μ > 70 (one-sided)\n",
    "\n",
    "# 2. Calculate test statistic\n",
    "t_stat = (x_bar - mu_0) / (s / np.sqrt(n))\n",
    "print(f\"Test statistic: t = {t_stat:.3f}\")\n",
    "\n",
    "# 3. Calculate p-value (one-sided)\n",
    "df = n - 1\n",
    "p_value = 1 - stats.t.cdf(t_stat, df)\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "# 4. Decision at α = 0.05\n",
    "alpha = 0.05\n",
    "decision = \"Reject H₀\" if p_value < alpha else \"Fail to reject H₀\"\n",
    "print(f\"\\nDecision at α = {alpha}: {decision}\")\n",
    "print(f\"Conclusion: There {'is' if p_value < alpha else 'is not'} significant evidence that the new method increases scores.\")\n",
    "\n",
    "# 5. Visualize null distribution\n",
    "x = np.linspace(-4, 4, 200)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, stats.t(df).pdf(x), 'b-', linewidth=2, label='Null distribution t(35)')\n",
    "plt.axvline(t_stat, color='red', linestyle='--', linewidth=2, label=f'Observed t = {t_stat:.2f}')\n",
    "\n",
    "# Shade rejection region\n",
    "t_crit = stats.t.ppf(1 - alpha, df)\n",
    "x_reject = np.linspace(t_crit, 4, 100)\n",
    "plt.fill_between(x_reject, 0, stats.t(df).pdf(x_reject), alpha=0.3, color='red', label='Rejection region')\n",
    "\n",
    "plt.xlabel('t-value')\n",
    "plt.ylabel('Density')\n",
    "plt.title('One-sided t-test: H₀: μ = 70 vs H₁: μ > 70')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Advanced Statistical Problems\n",
    "\n",
    "### Solution 1.6: Power Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "mu_0 = 100\n",
    "mu_1 = 105  # true mean under H₁\n",
    "sigma = 15\n",
    "n = 25\n",
    "alpha = 0.05\n",
    "\n",
    "# 2. Critical value for rejecting H₀ (one-sided test)\n",
    "z_alpha = stats.norm.ppf(1 - alpha)\n",
    "critical_value = mu_0 + z_alpha * (sigma / np.sqrt(n))\n",
    "print(f\"Critical value: {critical_value:.2f}\")\n",
    "print(f\"Reject H₀ if X̄ > {critical_value:.2f}\")\n",
    "\n",
    "# 3. Probability of Type II error when μ = 105\n",
    "# β = P(X̄ < critical_value | μ = 105)\n",
    "beta = stats.norm.cdf(critical_value, loc=mu_1, scale=sigma/np.sqrt(n))\n",
    "print(f\"\\nType II error (β): {beta:.4f}\")\n",
    "\n",
    "# 4. Power = 1 - β\n",
    "power = 1 - beta\n",
    "print(f\"Power: {power:.4f}\")\n",
    "\n",
    "# 5. Power curve for different true means\n",
    "true_means = np.linspace(100, 110, 50)\n",
    "powers = [1 - stats.norm.cdf(critical_value, loc=mu, scale=sigma/np.sqrt(n)) for mu in true_means]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(true_means, powers, 'b-', linewidth=2, label='n=25')\n",
    "plt.axhline(0.05, color='red', linestyle='--', label='α = 0.05 (Type I error)')\n",
    "plt.axvline(mu_0, color='gray', linestyle='--', label='H₀: μ = 100')\n",
    "plt.xlabel('True Mean (μ)')\n",
    "plt.ylabel('Power')\n",
    "plt.title('Power Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Compare with n=50\n",
    "critical_value_50 = mu_0 + z_alpha * (sigma / np.sqrt(50))\n",
    "powers_50 = [1 - stats.norm.cdf(critical_value_50, loc=mu, scale=sigma/np.sqrt(50)) for mu in true_means]\n",
    "plt.plot(true_means, powers_50, 'g-', linewidth=2, label='n=50')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPower at μ=105: n=25: {power:.3f}, n=50: {powers_50[25]:.3f}\")\n",
    "print(\"Increasing sample size increases power.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1.7: Comparing Two Populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generate data for one experiment\n",
    "n_A, n_B = 40, 40\n",
    "data_A = np.random.normal(1000, 100, n_A)\n",
    "data_B = np.random.normal(1050, 100, n_B)\n",
    "\n",
    "# 2. Two-sample t-test\n",
    "t_stat, p_value = stats.ttest_ind(data_B, data_A)\n",
    "print(f\"t-statistic: {t_stat:.3f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "print(f\"Conclusion: {'Reject H₀' if p_value < 0.05 else 'Fail to reject H₀'}\")\n",
    "\n",
    "# 3. 95% CI for difference in means\n",
    "mean_diff = data_B.mean() - data_A.mean()\n",
    "se_diff = np.sqrt(data_A.var(ddof=1)/n_A + data_B.var(ddof=1)/n_B)\n",
    "df = n_A + n_B - 2\n",
    "t_crit = stats.t.ppf(0.975, df)\n",
    "ci_lower = mean_diff - t_crit * se_diff\n",
    "ci_upper = mean_diff + t_crit * se_diff\n",
    "print(f\"\\n95% CI for μ_B - μ_A: [{ci_lower:.2f}, {ci_upper:.2f}]\")\n",
    "\n",
    "# 4. Repeat 1000 times\n",
    "n_simulations = 1000\n",
    "rejections = 0\n",
    "p_values_list = []\n",
    "\n",
    "for _ in range(n_simulations):\n",
    "    data_A = np.random.normal(1000, 100, n_A)\n",
    "    data_B = np.random.normal(1050, 100, n_B)  # true difference = 50\n",
    "    t_stat, p_val = stats.ttest_ind(data_B, data_A)\n",
    "    p_values_list.append(p_val)\n",
    "    if p_val < 0.05:\n",
    "        rejections += 1\n",
    "\n",
    "empirical_power = rejections / n_simulations\n",
    "print(f\"\\nEmpirical power: {empirical_power:.3f}\")\n",
    "\n",
    "# Distribution of p-values when H₀ is false\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(p_values_list, bins=50, density=True, alpha=0.7, edgecolor='black')\n",
    "plt.axvline(0.05, color='red', linestyle='--', linewidth=2, label='α = 0.05')\n",
    "plt.xlabel('P-value')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of P-values when H₁ is True')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"P-values skewed toward 0 when alternative hypothesis is true.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1.8: Chi-Squared Distribution and Sample Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "true_variance = 100\n",
    "n = 20\n",
    "df = n - 1\n",
    "\n",
    "# 1-2. Generate samples and calculate standardized quantities\n",
    "standardized_variances = []\n",
    "for _ in range(5000):\n",
    "    sample = np.random.normal(50, np.sqrt(true_variance), n)\n",
    "    s_squared = sample.var(ddof=1)\n",
    "    standardized = (n - 1) * s_squared / true_variance\n",
    "    standardized_variances.append(standardized)\n",
    "\n",
    "# 3-4. Plot histogram with theoretical χ²(19)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(standardized_variances, bins=50, density=True, alpha=0.7, edgecolor='black', label='Empirical')\n",
    "\n",
    "x = np.linspace(0, 50, 200)\n",
    "plt.plot(x, stats.chi2(df).pdf(x), 'r-', linewidth=2, label=f'Theoretical χ²({df})')\n",
    "\n",
    "plt.xlabel('(n-1)s²/σ²')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of Standardized Sample Variance')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 5. 95% CI for σ²\n",
    "sample = np.random.normal(50, np.sqrt(true_variance), n)\n",
    "s_squared = sample.var(ddof=1)\n",
    "\n",
    "# Critical values from χ² distribution\n",
    "chi2_lower = stats.chi2.ppf(0.025, df)\n",
    "chi2_upper = stats.chi2.ppf(0.975, df)\n",
    "\n",
    "# CI for σ²: [(n-1)s²/χ²_upper, (n-1)s²/χ²_lower]\n",
    "ci_lower = (n - 1) * s_squared / chi2_upper\n",
    "ci_upper = (n - 1) * s_squared / chi2_lower\n",
    "\n",
    "print(f\"\\nSample variance: s² = {s_squared:.2f}\")\n",
    "print(f\"95% CI for σ²: [{ci_lower:.2f}, {ci_upper:.2f}]\")\n",
    "print(f\"True σ² = {true_variance} {'is' if ci_lower <= true_variance <= ci_upper else 'is not'} in CI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1.9: Multiple Testing Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_tests = 20\n",
    "alpha = 0.05\n",
    "n_simulations = 1000\n",
    "n_samples = 30\n",
    "\n",
    "# 1-4. Simulate multiple testing scenarios\n",
    "at_least_one_rejection = 0\n",
    "\n",
    "for _ in range(n_simulations):\n",
    "    # Run 20 tests where all H₀ are true\n",
    "    rejections_this_sim = 0\n",
    "    \n",
    "    for test in range(n_tests):\n",
    "        # Generate data under H₀: μ = 0\n",
    "        data = np.random.normal(0, 1, n_samples)\n",
    "        t_stat, p_val = stats.ttest_1samp(data, 0)\n",
    "        \n",
    "        if p_val < alpha:\n",
    "            rejections_this_sim += 1\n",
    "    \n",
    "    if rejections_this_sim > 0:\n",
    "        at_least_one_rejection += 1\n",
    "\n",
    "# 5. Calculate family-wise error rate\n",
    "fwer_empirical = at_least_one_rejection / n_simulations\n",
    "fwer_theoretical = 1 - (1 - alpha)**n_tests\n",
    "\n",
    "print(f\"Empirical FWER: {fwer_empirical:.3f}\")\n",
    "print(f\"Theoretical FWER: {fwer_theoretical:.3f}\")\n",
    "print(f\"Expected under independence: {fwer_theoretical:.3f}\")\n",
    "\n",
    "# 6. Apply Bonferroni correction\n",
    "alpha_bonf = alpha / n_tests\n",
    "at_least_one_bonf = 0\n",
    "\n",
    "for _ in range(n_simulations):\n",
    "    rejections_this_sim = 0\n",
    "    \n",
    "    for test in range(n_tests):\n",
    "        data = np.random.normal(0, 1, n_samples)\n",
    "        t_stat, p_val = stats.ttest_1samp(data, 0)\n",
    "        \n",
    "        if p_val < alpha_bonf:\n",
    "            rejections_this_sim += 1\n",
    "    \n",
    "    if rejections_this_sim > 0:\n",
    "        at_least_one_bonf += 1\n",
    "\n",
    "fwer_bonf = at_least_one_bonf / n_simulations\n",
    "print(f\"\\nBonferroni correction (α = {alpha_bonf:.4f}):\")\n",
    "print(f\"Empirical FWER: {fwer_bonf:.3f} (target: {alpha})\")\n",
    "print(\"Bonferroni controls family-wise error rate at nominal level.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1.10: Sample Size Determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given\n",
    "E = 2  # margin of error\n",
    "sigma = 12\n",
    "alpha = 0.05\n",
    "z_alpha = stats.norm.ppf(1 - alpha/2)\n",
    "\n",
    "# 3. Calculate required sample size\n",
    "n_required = ((z_alpha * sigma) / E)**2\n",
    "n_required = int(np.ceil(n_required))\n",
    "print(f\"Required sample size for E={E}: n = {n_required}\")\n",
    "\n",
    "# 4. Verify by simulation\n",
    "ci_widths = []\n",
    "for _ in range(1000):\n",
    "    sample = np.random.normal(100, sigma, n_required)\n",
    "    mean = sample.mean()\n",
    "    std = sample.std(ddof=1)\n",
    "    t_crit = stats.t.ppf(0.975, n_required-1)\n",
    "    margin = t_crit * (std / np.sqrt(n_required))\n",
    "    ci_widths.append(2 * margin)  # total width\n",
    "\n",
    "avg_width = np.mean(ci_widths)\n",
    "print(f\"Average CI width: {avg_width:.2f} (target: {2*E})\")\n",
    "\n",
    "# 5. Sample size for different margins\n",
    "print(\"\\nRequired sample sizes for different margins of error:\")\n",
    "for E_new in [2, 1, 0.5]:\n",
    "    n_new = int(np.ceil(((z_alpha * sigma) / E_new)**2))\n",
    "    print(f\"E = {E_new}: n = {n_new}\")\n",
    "\n",
    "# Visualize relationship\n",
    "margins = np.linspace(0.5, 5, 50)\n",
    "sample_sizes = [((z_alpha * sigma) / m)**2 for m in margins]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(margins, sample_sizes, 'b-', linewidth=2)\n",
    "plt.xlabel('Margin of Error (E)')\n",
    "plt.ylabel('Required Sample Size (n)')\n",
    "plt.title('Sample Size vs Margin of Error')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "print(\"\\nSample size increases quadratically as margin of error decreases.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: AI/Machine Learning Applications\n",
    "\n",
    "### Solution 1.11: Train-Test Split and Sampling Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create dataset with known relationship\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "x = np.random.uniform(0, 10, n)\n",
    "y = 2*x + 3 + np.random.normal(0, 2, n)  # y = 2x + 3 + noise\n",
    "\n",
    "# 2-3. Perform 100 different splits and evaluate\n",
    "train_r2_scores = []\n",
    "test_r2_scores = []\n",
    "\n",
    "for seed in range(100):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        x.reshape(-1, 1), y, test_size=0.2, random_state=seed\n",
    "    )\n",
    "    \n",
    "    # Fit linear model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate on both sets\n",
    "    train_r2 = model.score(X_train, y_train)\n",
    "    test_r2 = model.score(X_test, y_test)\n",
    "    \n",
    "    train_r2_scores.append(train_r2)\n",
    "    test_r2_scores.append(test_r2)\n",
    "\n",
    "# 4. Plot distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(train_r2_scores, bins=20, alpha=0.7, edgecolor='black', label='Train R²')\n",
    "axes[0].axvline(np.mean(train_r2_scores), color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "axes[0].set_xlabel('R² Score')\n",
    "axes[0].set_title('Training R² Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(test_r2_scores, bins=20, alpha=0.7, edgecolor='black', label='Test R²', color='orange')\n",
    "axes[1].axvline(np.mean(test_r2_scores), color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "axes[1].set_xlabel('R² Score')\n",
    "axes[1].set_title('Test R² Distribution')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Calculate statistics\n",
    "print(f\"Training R² - Mean: {np.mean(train_r2_scores):.4f}, Std: {np.std(train_r2_scores):.4f}\")\n",
    "print(f\"Test R² - Mean: {np.mean(test_r2_scores):.4f}, Std: {np.std(test_r2_scores):.4f}\")\n",
    "\n",
    "# 6. Interpretation\n",
    "print(\"\\nObservations:\")\n",
    "print(\"1. Test R² has higher variance than training R² (smaller sample size)\")\n",
    "print(\"2. Both distributions are approximately normal (CLT)\")\n",
    "print(\"3. Training R² slightly higher due to overfitting\")\n",
    "print(\"4. Understanding this variability is crucial for model evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1.12: Bias-Variance Decomposition in Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generate data from true function\n",
    "np.random.seed(42)\n",
    "x_true = np.linspace(0, 1, 100)\n",
    "f_true = np.sin(2 * np.pi * x_true)  # true function\n",
    "noise_std = 0.1\n",
    "\n",
    "# Test point for bias-variance calculation\n",
    "x_test = 0.5\n",
    "y_test_true = np.sin(2 * np.pi * x_test)\n",
    "\n",
    "# 2. For each polynomial degree\n",
    "degrees = [1, 2, 5, 10, 20]\n",
    "bias_squared = []\n",
    "variance = []\n",
    "mse = []\n",
    "\n",
    "for d in degrees:\n",
    "    predictions = []\n",
    "    \n",
    "    # Generate 100 training datasets\n",
    "    for _ in range(100):\n",
    "        # Random training data\n",
    "        x_train = np.random.uniform(0, 1, 50)\n",
    "        y_train = np.sin(2 * np.pi * x_train) + np.random.normal(0, noise_std, 50)\n",
    "        \n",
    "        # Fit polynomial\n",
    "        coeffs = np.polyfit(x_train, y_train, d)\n",
    "        pred = np.polyval(coeffs, x_test)\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    # 3. Calculate bias² and variance\n",
    "    mean_pred = np.mean(predictions)\n",
    "    bias_sq = (mean_pred - y_test_true)**2\n",
    "    var = np.var(predictions)\n",
    "    total_mse = bias_sq + var + noise_std**2\n",
    "    \n",
    "    bias_squared.append(bias_sq)\n",
    "    variance.append(var)\n",
    "    mse.append(total_mse)\n",
    "\n",
    "# 4. Plot bias-variance tradeoff\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(degrees, bias_squared, 'o-', label='Bias²', linewidth=2)\n",
    "plt.plot(degrees, variance, 's-', label='Variance', linewidth=2)\n",
    "plt.plot(degrees, mse, '^-', label='Total MSE', linewidth=2)\n",
    "plt.xlabel('Model Complexity (Polynomial Degree)')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Bias-Variance Tradeoff')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# 5. Identify optimal complexity\n",
    "optimal_d = degrees[np.argmin(mse)]\n",
    "print(f\"\\nOptimal polynomial degree: {optimal_d}\")\n",
    "print(f\"MSE decomposition at optimal:\")\n",
    "print(f\"  Bias²: {bias_squared[degrees.index(optimal_d)]:.4f}\")\n",
    "print(f\"  Variance: {variance[degrees.index(optimal_d)]:.4f}\")\n",
    "print(f\"  Noise: {noise_std**2:.4f}\")\n",
    "print(f\"  Total MSE: {mse[degrees.index(optimal_d)]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1.13: Confidence Intervals for Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, \n",
    "                          n_redundant=5, random_state=42)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=200, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 2. Evaluate on test set\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "n_test = len(y_test)\n",
    "\n",
    "# 3. Confidence intervals\n",
    "# Normal approximation\n",
    "z_crit = stats.norm.ppf(0.975)\n",
    "se = np.sqrt(accuracy * (1 - accuracy) / n_test)\n",
    "ci_normal_lower = accuracy - z_crit * se\n",
    "ci_normal_upper = accuracy + z_crit * se\n",
    "\n",
    "# Wilson score interval (better for proportions)\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "n_correct = int(accuracy * n_test)\n",
    "ci_wilson = proportion_confint(n_correct, n_test, alpha=0.05, method='wilson')\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"\\n95% Confidence Intervals:\")\n",
    "print(f\"Normal approx: [{ci_normal_lower:.4f}, {ci_normal_upper:.4f}]\")\n",
    "print(f\"Wilson score: [{ci_wilson[0]:.4f}, {ci_wilson[1]:.4f}]\")\n",
    "\n",
    "# 4-5. Verify coverage\n",
    "coverages_normal = []\n",
    "coverages_wilson = []\n",
    "\n",
    "for seed in range(1000):\n",
    "    # Generate new data\n",
    "    X, y = make_classification(n_samples=1000, n_features=20, random_state=seed)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=200, random_state=seed)\n",
    "    \n",
    "    # Train and get population accuracy\n",
    "    model = LogisticRegression(max_iter=1000, random_state=seed)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # True population accuracy (evaluate on large test set)\n",
    "    X_pop, y_pop = make_classification(n_samples=10000, n_features=20, random_state=seed)\n",
    "    true_acc = model.score(X_pop, y_pop)\n",
    "    \n",
    "    # Observed accuracy on small test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    obs_acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Construct CIs\n",
    "    se = np.sqrt(obs_acc * (1 - obs_acc) / n_test)\n",
    "    ci_norm = (obs_acc - z_crit * se, obs_acc + z_crit * se)\n",
    "    \n",
    "    n_correct = int(obs_acc * n_test)\n",
    "    ci_wils = proportion_confint(n_correct, n_test, alpha=0.05, method='wilson')\n",
    "    \n",
    "    # Check coverage\n",
    "    coverages_normal.append(ci_norm[0] <= true_acc <= ci_norm[1])\n",
    "    coverages_wilson.append(ci_wils[0] <= true_acc <= ci_wils[1])\n",
    "\n",
    "print(f\"\\nEmpirical Coverage Rates (1000 repetitions):\")\n",
    "print(f\"Normal approximation: {np.mean(coverages_normal):.3f} (target: 0.95)\")\n",
    "print(f\"Wilson score: {np.mean(coverages_wilson):.3f} (target: 0.95)\")\n",
    "\n",
    "# 6. Effect of sample size\n",
    "print(f\"\\nCI width vs sample size:\")\n",
    "for n in [50, 100, 200, 500, 1000]:\n",
    "    se = np.sqrt(0.8 * 0.2 / n)  # assume accuracy=0.8\n",
    "    width = 2 * z_crit * se\n",
    "    print(f\"n={n:4d}: width = {width:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1.14: Hypothesis Testing for Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Train two models\n",
    "X, y = make_classification(n_samples=500, n_features=20, n_informative=15, random_state=42)\n",
    "\n",
    "model1 = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model2 = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "\n",
    "# 2. 10-fold cross-validation\n",
    "scores1 = cross_val_score(model1, X, y, cv=10, scoring='accuracy')\n",
    "scores2 = cross_val_score(model2, X, y, cv=10, scoring='accuracy')\n",
    "\n",
    "print(f\"Logistic Regression: {scores1.mean():.4f} ± {scores1.std():.4f}\")\n",
    "print(f\"Random Forest: {scores2.mean():.4f} ± {scores2.std():.4f}\")\n",
    "\n",
    "# 3. Paired t-test\n",
    "differences = scores2 - scores1\n",
    "t_stat, p_value = stats.ttest_rel(scores2, scores1)\n",
    "\n",
    "print(f\"\\nPaired t-test:\")\n",
    "print(f\"Mean difference: {differences.mean():.4f}\")\n",
    "print(f\"t-statistic: {t_stat:.3f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "# 4. Effect size (Cohen's d)\n",
    "cohens_d = differences.mean() / differences.std()\n",
    "print(f\"Cohen's d: {cohens_d:.3f}\")\n",
    "\n",
    "# Interpretation\n",
    "if p_value < 0.05:\n",
    "    print(f\"\\nConclusion: Random Forest performs significantly {'better' if differences.mean() > 0 else 'worse'} than Logistic Regression\")\n",
    "else:\n",
    "    print(f\"\\nConclusion: No significant difference between models\")\n",
    "\n",
    "# Visualize differences\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot([scores1, scores2], labels=['Logistic Regression', 'Random Forest'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Performance Comparison (10-fold CV)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# 5. Why paired test?\n",
    "print(\"\\nWhy use paired test:\")\n",
    "print(\"- Same folds used for both models (matched pairs)\")\n",
    "print(\"- Removes variation due to different train/test splits\")\n",
    "print(\"- More powerful than independent samples test\")\n",
    "print(\"- Controls for fold-to-fold variability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1.15: P-values and Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generate dataset with sparse signal\n",
    "np.random.seed(42)\n",
    "n = 200\n",
    "p = 100\n",
    "\n",
    "# True predictive features: first 5\n",
    "X = np.random.randn(n, p)\n",
    "true_beta = np.zeros(p)\n",
    "true_beta[:5] = [2, -1.5, 1, -2, 1.5]  # only first 5 features matter\n",
    "y = X @ true_beta + np.random.randn(n)\n",
    "\n",
    "# 2. Test each feature individually\n",
    "from scipy.stats import pearsonr\n",
    "p_values = []\n",
    "for j in range(p):\n",
    "    _, p_val = pearsonr(X[:, j], y)\n",
    "    p_values.append((j, p_val))\n",
    "\n",
    "# Sort by p-value\n",
    "p_values.sort(key=lambda x: x[1])\n",
    "\n",
    "# 3. Select features with p < 0.05\n",
    "selected_uncorrected = [feat for feat, pval in p_values if pval < 0.05]\n",
    "print(f\"Selected features (uncorrected): {selected_uncorrected}\")\n",
    "\n",
    "# 4. Count false positives\n",
    "true_features = set(range(5))\n",
    "selected_set = set(selected_uncorrected)\n",
    "false_positives = selected_set - true_features\n",
    "false_negatives = true_features - selected_set\n",
    "\n",
    "print(f\"\\nTrue features: {sorted(true_features)}\")\n",
    "print(f\"False positives: {sorted(false_positives)} (count: {len(false_positives)})\")\n",
    "print(f\"False negatives: {sorted(false_negatives)} (count: {len(false_negatives)})\")\n",
    "\n",
    "# 5. Benjamini-Hochberg FDR control\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "_, p_values_sorted, _, _ = multipletests([pv for _, pv in p_values], alpha=0.05, method='fdr_bh')\n",
    "\n",
    "# Reconstruct feature selection with FDR\n",
    "selected_fdr = [feat for feat, pval in zip([f for f, _ in p_values], p_values_sorted) if pval < 0.05]\n",
    "\n",
    "print(f\"\\nSelected features (FDR-corrected): {selected_fdr}\")\n",
    "\n",
    "# 6. Compare selections\n",
    "selected_fdr_set = set(selected_fdr)\n",
    "false_positives_fdr = selected_fdr_set - true_features\n",
    "false_negatives_fdr = true_features - selected_fdr_set\n",
    "\n",
    "print(f\"False positives (FDR): {sorted(false_positives_fdr)} (count: {len(false_positives_fdr)})\")\n",
    "print(f\"False negatives (FDR): {sorted(false_negatives_fdr)} (count: {len(false_negatives_fdr)})\")\n",
    "\n",
    "# Visualize p-values\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "colors = ['red' if i < 5 else 'gray' for i in range(p)]\n",
    "plt.scatter(range(p), [pv for _, pv in sorted(p_values, key=lambda x: x[0])], c=colors, alpha=0.6)\n",
    "plt.axhline(0.05, color='blue', linestyle='--', label='α = 0.05')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('P-value')\n",
    "plt.title('P-values for Each Feature (red = true signal)')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "comparison = ['Uncorrected', 'FDR']\n",
    "fp_counts = [len(false_positives), len(false_positives_fdr)]\n",
    "fn_counts = [len(false_negatives), len(false_negatives_fdr)]\n",
    "x = np.arange(len(comparison))\n",
    "plt.bar(x - 0.2, fp_counts, 0.4, label='False Positives', color='red', alpha=0.7)\n",
    "plt.bar(x + 0.2, fn_counts, 0.4, label='False Negatives', color='blue', alpha=0.7)\n",
    "plt.xticks(x, comparison)\n",
    "plt.ylabel('Count')\n",
    "plt.title('Error Comparison')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFDR correction reduces false positives while maintaining power.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Contemporary Problems (2025+)\n",
    "\n",
    "**Note**: Solutions 1.16-1.20 involve advanced topics requiring simulation of modern ML systems. Complete implementations would require additional libraries and extended code. Below are conceptual solution outlines with key statistical techniques.\n",
    "\n",
    "### Solution 1.16: Uncertainty Quantification in LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate LLM classification task\n",
    "n_samples = 1000\n",
    "n_classes = 4\n",
    "\n",
    "# Generate softmax probabilities (simulating LLM output)\n",
    "np.random.seed(42)\n",
    "logits = np.random.randn(n_samples, n_classes)\n",
    "probs = np.exp(logits) / np.exp(logits).sum(axis=1, keepdims=True)\n",
    "\n",
    "# True labels (simulate)\n",
    "true_labels = np.random.choice(n_classes, n_samples)\n",
    "predicted_labels = probs.argmax(axis=1)\n",
    "\n",
    "# Calculate predictive entropy\n",
    "entropy = -np.sum(probs * np.log(probs + 1e-10), axis=1)\n",
    "max_prob = probs.max(axis=1)\n",
    "\n",
    "print(f\"Entropy range: [{entropy.min():.3f}, {entropy.max():.3f}]\")\n",
    "print(f\"Max prob range: [{max_prob.min():.3f}, {max_prob.max():.3f}]\")\n",
    "\n",
    "# Establish confidence threshold\n",
    "threshold = np.percentile(entropy, 25)  # bottom 25% entropy = high confidence\n",
    "high_confidence = entropy < threshold\n",
    "\n",
    "print(f\"\\nHigh confidence predictions: {high_confidence.sum()}/{n_samples}\")\n",
    "print(f\"Accuracy (high conf): {(predicted_labels[high_confidence] == true_labels[high_confidence]).mean():.3f}\")\n",
    "print(f\"Accuracy (low conf): {(predicted_labels[~high_confidence] == true_labels[~high_confidence]).mean():.3f}\")\n",
    "\n",
    "# Calibration plot\n",
    "n_bins = 10\n",
    "prob_bins = np.linspace(0, 1, n_bins + 1)\n",
    "bin_accuracies = []\n",
    "bin_confidences = []\n",
    "\n",
    "for i in range(n_bins):\n",
    "    mask = (max_prob >= prob_bins[i]) & (max_prob < prob_bins[i+1])\n",
    "    if mask.sum() > 0:\n",
    "        bin_acc = (predicted_labels[mask] == true_labels[mask]).mean()\n",
    "        bin_conf = max_prob[mask].mean()\n",
    "        bin_accuracies.append(bin_acc)\n",
    "        bin_confidences.append(bin_conf)\n",
    "\n",
    "# Plot calibration\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Perfect calibration')\n",
    "plt.plot(bin_confidences, bin_accuracies, 'o-', linewidth=2, markersize=8, label='Model')\n",
    "plt.xlabel('Confidence')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Calibration Plot')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFor LLM outputs, confidence intervals can be constructed via:\")\n",
    "print(\"1. Ensemble methods (multiple models)\")\n",
    "print(\"2. Monte Carlo dropout\")\n",
    "print(\"3. Bayesian approximations\")\n",
    "print(\"4. Conformal prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1.17-1.20: Advanced Contemporary Problems\n",
    "\n",
    "**Solutions for exercises 1.17-1.20 follow similar patterns:**\n",
    "\n",
    "**Key techniques demonstrated:**\n",
    "- Simulation of complex systems (networks, neural networks, reward models)\n",
    "- Bootstrap and resampling for uncertainty quantification\n",
    "- Adjustment for bias and confounding\n",
    "- Construction of confidence intervals under non-standard conditions\n",
    "- Hypothesis testing with modern metrics\n",
    "\n",
    "**Statistical principles applied:**\n",
    "- Sampling distributions remain fundamental\n",
    "- Central Limit Theorem enables inference at scale\n",
    "- Bootstrap provides distribution-free uncertainty estimates\n",
    "- Proper experimental design controls for confounding\n",
    "- Multiple testing corrections prevent false discoveries\n",
    "\n",
    "**Note**: Full implementations would require:\n",
    "- Network libraries (networkx)\n",
    "- Deep learning frameworks (PyTorch/TensorFlow)\n",
    "- Advanced statistical packages (statsmodels, sklearn)\n",
    "- Causal inference tools (DoWhy, EconML)\n",
    "\n",
    "The core statistical inference principles from Chapter 1 apply directly to these modern problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 20px; border-radius: 10px; color: white; text-align: center;'>\n",
    "<p style='margin: 0; font-size: 1.1em;'>Solutions by <strong>PLAI Academy</strong></p>\n",
    "<p style='margin: 5px 0 0 0; opacity: 0.8;'>Statistical Inference • 2025</p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
