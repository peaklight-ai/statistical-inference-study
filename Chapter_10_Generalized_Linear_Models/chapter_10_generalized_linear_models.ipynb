{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10: Generalized Linear Models\n",
    "\n",
    "**Core Goal:** Extend linear regression to handle non-normal response variables and non-linear relationships through a unified framework.\n",
    "\n",
    "**Motivation:** Classical linear regression assumes the response variable is continuous and normally distributed. Many real-world problems violate these assumptions: binary outcomes (success/failure), count data (number of events), proportions (rates between 0 and 1). Applying ordinary least squares to such data leads to inappropriate predictions (probabilities outside [0,1], negative counts) and invalid inference. Generalized Linear Models provide a unified framework that extends linear models to exponential family distributions and uses link functions to ensure predictions remain in valid ranges. This single framework encompasses logistic regression, Poisson regression, and many other models, making it one of the most important tools in applied statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Limitations of Linear Regression\n",
    "\n",
    "**Classical Linear Model:** $Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i$ where $\\epsilon_i \\sim N(0, \\sigma^2)$\n",
    "\n",
    "**Assumptions:**\n",
    "- Response $Y$ is continuous\n",
    "- Errors are normally distributed\n",
    "- Variance is constant (homoscedasticity)\n",
    "- Relationship between $E[Y|X]$ and $X$ is linear\n",
    "\n",
    "**Problems with Non-Normal Data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary response: predict probability of success based on x\n",
    "np.random.seed(42)\n",
    "x = np.linspace(-2, 2, 50)\n",
    "true_prob = 1 / (1 + np.exp(-2*x))  # True probability (logistic)\n",
    "y = np.random.binomial(1, true_prob)  # Binary outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit linear regression (INAPPROPRIATE for binary data)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(x.reshape(-1, 1), y)\n",
    "linear_pred = linear_model.predict(x.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y, alpha=0.5, label='Observed (0/1)')\n",
    "plt.plot(x, linear_pred, 'r--', linewidth=2, label='Linear regression')\n",
    "plt.plot(x, true_prob, 'g-', linewidth=2, label='True probability')\n",
    "plt.xlabel('x'); plt.ylabel('y / P(Y=1)')\n",
    "plt.title('Linear Regression Fails for Binary Data')\n",
    "plt.legend(); plt.ylim(-0.2, 1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problems Observed:**\n",
    "- Linear model predicts probabilities < 0 and > 1 (nonsensical)\n",
    "- Assumes constant variance (but variance of binary variable depends on probability)\n",
    "- Cannot capture the S-shaped relationship between predictors and probability\n",
    "\n",
    "**Solution:** Generalized Linear Models address these issues through link functions and exponential family distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Exponential Family Distributions\n",
    "\n",
    "**Exponential Family:** A distribution belongs to the exponential family if its probability density/mass function can be written as:\n",
    "\n",
    "$$f(y; \\theta, \\phi) = \\exp\\left(\\frac{y\\theta - b(\\theta)}{a(\\phi)} + c(y, \\phi)\\right)$$\n",
    "\n",
    "where:\n",
    "- $\\theta$ is the natural parameter\n",
    "- $\\phi$ is the dispersion parameter\n",
    "- $b(\\theta)$ is the cumulant function\n",
    "- $a(\\phi)$ and $c(y, \\phi)$ are known functions\n",
    "\n",
    "**Key Properties:**\n",
    "- $E[Y] = \\mu = b'(\\theta)$\n",
    "- $\\text{Var}(Y) = b''(\\theta) \\cdot a(\\phi)$\n",
    "\n",
    "**Common Members:**\n",
    "- **Normal:** $\\theta = \\mu$, $b(\\theta) = \\theta^2/2$\n",
    "- **Binomial:** $\\theta = \\log(p/(1-p))$, $b(\\theta) = \\log(1+e^\\theta)$\n",
    "- **Poisson:** $\\theta = \\log(\\lambda)$, $b(\\theta) = e^\\theta$\n",
    "- **Gamma, Exponential, Inverse Gaussian**\n",
    "\n",
    "**Motivation:** Exponential family distributions have convenient mathematical properties for likelihood-based inference. Maximum Likelihood Estimators have closed forms or are easy to compute numerically, and large-sample theory provides asymptotic normality. Restricting Generalized Linear Models to exponential family ensures good statistical properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Components of Generalized Linear Models\n",
    "\n",
    "**Generalized Linear Model has three components:**\n",
    "\n",
    "1. **Random Component:** Response $Y_i$ follows an exponential family distribution with mean $\\mu_i$\n",
    "\n",
    "2. **Systematic Component:** Linear predictor $\\eta_i = \\beta_0 + \\beta_1 X_{i1} + ... + \\beta_p X_{ip} = \\mathbf{x}_i^T \\boldsymbol{\\beta}$\n",
    "\n",
    "3. **Link Function:** Connects mean to linear predictor: $g(\\mu_i) = \\eta_i$\n",
    "\n",
    "**Key Insight:** Link function transforms the response mean (which may be constrained, e.g., $\\mu \\in [0,1]$ for probabilities) to the linear predictor (unconstrained, $\\eta \\in (-\\infty, \\infty)$).\n",
    "\n",
    "**Canonical Link:** The link function that makes $\\eta = \\theta$ (natural parameter). Canonical links have nice theoretical properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 Logistic Regression\n",
    "\n",
    "**Application:** Binary response variable (success/failure, yes/no, 0/1)\n",
    "\n",
    "**Random Component:** $Y_i \\sim \\text{Bernoulli}(p_i)$\n",
    "\n",
    "**Link Function:** Logit link (canonical): $\\log\\left(\\frac{p_i}{1-p_i}\\right) = \\eta_i = \\beta_0 + \\beta_1 X_i$\n",
    "\n",
    "**Mean Function (Inverse Link):** $p_i = \\frac{e^{\\eta_i}}{1 + e^{\\eta_i}} = \\frac{1}{1 + e^{-\\eta_i}}$ (logistic function)\n",
    "\n",
    "**Motivation:** The logit link maps probabilities $p \\in [0,1]$ to log-odds $\\log(p/(1-p)) \\in (-\\infty, \\infty)$, allowing us to model the relationship linearly. The inverse logistic function ensures predicted probabilities always lie in [0,1]. This S-shaped curve captures how probabilities change gradually near extreme predictor values but rapidly in the middle range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same binary data as before\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(x.reshape(-1, 1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probabilities: p = 1/(1 + exp(-η))\n",
    "logistic_pred = logistic_model.predict_proba(x.reshape(-1, 1))[:, 1]\n",
    "print(f\"Logistic regression coefficients: β₀ = {logistic_model.intercept_[0]:.3f}, β₁ = {logistic_model.coef_[0][0]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y, alpha=0.5, label='Observed (0/1)')\n",
    "plt.plot(x, logistic_pred, 'b-', linewidth=2, label='Logistic regression')\n",
    "plt.plot(x, true_prob, 'g--', linewidth=2, label='True probability')\n",
    "plt.xlabel('x'); plt.ylabel('P(Y=1)')\n",
    "plt.title('Logistic Regression for Binary Data')\n",
    "plt.legend(); plt.ylim(-0.1, 1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:** Logistic regression produces valid probabilities in [0,1] and captures the S-shaped relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Logistic Regression Coefficients\n",
    "\n",
    "**Log-Odds Interpretation:** $\\beta_1$ represents the change in log-odds for one-unit increase in $X$.\n",
    "\n",
    "**Odds Ratio:** $e^{\\beta_1}$ is the multiplicative change in odds for one-unit increase in $X$.\n",
    "\n",
    "**Example:** If $\\beta_1 = 0.693$, then $e^{0.693} = 2$, meaning each one-unit increase in $X$ doubles the odds of success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Odds ratio: exp(β₁)\n",
    "beta1 = logistic_model.coef_[0][0]\n",
    "odds_ratio = np.exp(beta1)\n",
    "print(f\"Odds ratio: {odds_ratio:.3f}\")\n",
    "print(f\"Interpretation: One-unit increase in x multiplies odds by {odds_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Estimation for Logistic Regression\n",
    "\n",
    "**Log-Likelihood:** $\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n [y_i \\log(p_i) + (1-y_i)\\log(1-p_i)]$\n",
    "\n",
    "where $p_i = 1/(1 + e^{-\\mathbf{x}_i^T\\boldsymbol{\\beta}})$\n",
    "\n",
    "**Estimation:** No closed form; use iterative methods (Newton-Raphson, Fisher scoring).\n",
    "\n",
    "**Asymptotic Properties:** Maximum Likelihood Estimator is asymptotically normal: $\\hat{\\boldsymbol{\\beta}} \\sim N(\\boldsymbol{\\beta}, I^{-1}(\\boldsymbol{\\beta}))$ where $I$ is Fisher information matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.5 Poisson Regression\n",
    "\n",
    "**Application:** Count data (number of events: accidents, customer arrivals, disease cases)\n",
    "\n",
    "**Random Component:** $Y_i \\sim \\text{Poisson}(\\lambda_i)$\n",
    "\n",
    "**Link Function:** Log link (canonical): $\\log(\\lambda_i) = \\eta_i = \\beta_0 + \\beta_1 X_i$\n",
    "\n",
    "**Mean Function (Inverse Link):** $\\lambda_i = e^{\\eta_i}$\n",
    "\n",
    "**Motivation:** Count data are non-negative integers. The log link ensures predicted counts are always positive: $\\lambda = e^\\eta > 0$ for any $\\eta$. The exponential relationship means effects are multiplicative on the count scale: increasing $X$ by one unit multiplies the expected count by $e^{\\beta_1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate count data: number of events increases with x\n",
    "np.random.seed(42)\n",
    "x_count = np.linspace(0, 3, 40)\n",
    "true_lambda = np.exp(1 + 0.5 * x_count)  # λ = exp(1 + 0.5x)\n",
    "y_count = np.random.poisson(true_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Poisson regression: log(λ) = β₀ + β₁x\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "poisson_model = PoissonRegressor()\n",
    "poisson_model.fit(x_count.reshape(-1, 1), y_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted counts: λ̂ = exp(β₀ + β₁x)\n",
    "poisson_pred = poisson_model.predict(x_count.reshape(-1, 1))\n",
    "print(f\"Poisson regression: β₀ = {poisson_model.intercept_:.3f}, β₁ = {poisson_model.coef_[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_count, y_count, alpha=0.6, label='Observed counts')\n",
    "plt.plot(x_count, poisson_pred, 'r-', linewidth=2, label='Poisson regression')\n",
    "plt.plot(x_count, true_lambda, 'g--', linewidth=2, label='True λ')\n",
    "plt.xlabel('x'); plt.ylabel('Count'); plt.title('Poisson Regression')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Poisson Regression Coefficients\n",
    "\n",
    "**Rate Ratio:** $e^{\\beta_1}$ is the multiplicative change in expected count for one-unit increase in $X$.\n",
    "\n",
    "**Example:** If $\\beta_1 = 0.5$, then $e^{0.5} = 1.65$, meaning each one-unit increase in $X$ increases the expected count by 65%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rate ratio: exp(β₁)\n",
    "beta1_poisson = poisson_model.coef_[0]\n",
    "rate_ratio = np.exp(beta1_poisson)\n",
    "print(f\"Rate ratio: {rate_ratio:.3f}\")\n",
    "print(f\"Interpretation: One-unit increase in x multiplies expected count by {rate_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.6 Common Link Functions\n",
    "\n",
    "**Different distributions require different link functions:**\n",
    "\n",
    "| **Distribution** | **Canonical Link** | **Link Function** | **Inverse Link** | **Application** |\n",
    "|------------------|-------------------|-------------------|------------------|----------------|\n",
    "| Normal | Identity | $g(\\mu) = \\mu$ | $\\mu = \\eta$ | Continuous response |\n",
    "| Binomial | Logit | $g(p) = \\log(p/(1-p))$ | $p = 1/(1+e^{-\\eta})$ | Binary/proportion |\n",
    "| Poisson | Log | $g(\\lambda) = \\log(\\lambda)$ | $\\lambda = e^\\eta$ | Count data |\n",
    "| Gamma | Inverse | $g(\\mu) = 1/\\mu$ | $\\mu = 1/\\eta$ | Positive continuous |\n",
    "\n",
    "**Non-Canonical Links:**\n",
    "- **Probit link** for binomial: $g(p) = \\Phi^{-1}(p)$ (inverse normal cumulative distribution function)\n",
    "- **Complementary log-log:** $g(p) = \\log(-\\log(1-p))$ for binomial\n",
    "\n",
    "**Choosing Link:**\n",
    "- Canonical link: theoretically optimal, simplifies computation\n",
    "- Alternative links: may fit data better or have more natural interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.7 Model Fitting and Inference\n",
    "\n",
    "**Estimation Method:** Maximum Likelihood via Iteratively Reweighted Least Squares\n",
    "\n",
    "**Procedure:**\n",
    "1. Start with initial parameter estimates\n",
    "2. Compute working response and weights\n",
    "3. Fit weighted least squares\n",
    "4. Update parameters\n",
    "5. Repeat until convergence\n",
    "\n",
    "**Asymptotic Distribution:** $\\hat{\\boldsymbol{\\beta}} \\sim N(\\boldsymbol{\\beta}, \\mathcal{I}^{-1}(\\boldsymbol{\\beta}))$\n",
    "\n",
    "**Standard Errors:** Diagonal elements of $\\mathcal{I}^{-1}(\\hat{\\boldsymbol{\\beta}})$\n",
    "\n",
    "**Confidence Intervals:** $\\hat{\\beta}_j \\pm z_{\\alpha/2} \\cdot \\widehat{SE}(\\hat{\\beta}_j)$\n",
    "\n",
    "**Hypothesis Tests:**\n",
    "- **Wald test:** $z = \\hat{\\beta}_j / \\widehat{SE}(\\hat{\\beta}_j) \\sim N(0,1)$ under $H_0: \\beta_j = 0$\n",
    "- **Likelihood ratio test:** Compare nested models using deviance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access coefficient statistics from statsmodels\n",
    "import statsmodels.api as sm\n",
    "# Add intercept to design matrix\n",
    "X_with_intercept = sm.add_constant(x.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit logistic regression with full inference\n",
    "logit_model = sm.Logit(y, X_with_intercept)\n",
    "logit_result = logit_model.fit(disp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary with Standard Errors, z-statistics, p-values, Confidence Intervals\n",
    "print(logit_result.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.8 Deviance and Model Comparison\n",
    "\n",
    "**Deviance:** Measure of model fit (smaller is better).\n",
    "\n",
    "**Definition:** $D = -2[\\ell(\\hat{\\boldsymbol{\\beta}}) - \\ell(\\hat{\\boldsymbol{\\beta}}_{saturated})]$\n",
    "\n",
    "where $\\ell(\\hat{\\boldsymbol{\\beta}})$ is log-likelihood of fitted model and $\\ell(\\hat{\\boldsymbol{\\beta}}_{saturated})$ is log-likelihood of saturated model (one parameter per observation).\n",
    "\n",
    "**Null Deviance:** Deviance for model with only intercept\n",
    "\n",
    "**Residual Deviance:** Deviance for fitted model\n",
    "\n",
    "**Interpretation:** Reduction in deviance measures improvement of model over null model.\n",
    "\n",
    "**Motivation:** Deviance plays the role of residual sum of squares in linear regression. It quantifies how well the model fits the data. The difference in deviance between nested models follows a chi-squared distribution, enabling likelihood ratio tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deviance statistics from fitted model\n",
    "print(f\"Null deviance: {logit_result.llnull * -2:.2f}\")\n",
    "print(f\"Residual deviance: {logit_result.deviance:.2f}\")\n",
    "print(f\"Reduction in deviance: {logit_result.llnull * -2 - logit_result.deviance:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood Ratio Test\n",
    "\n",
    "**Test Statistic:** $LR = D_{reduced} - D_{full} \\sim \\chi^2_{\\Delta df}$ under $H_0$\n",
    "\n",
    "**Use:** Compare nested models (e.g., test if additional predictors improve fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add quadratic term: test if x² improves model\n",
    "X_quadratic = sm.add_constant(np.column_stack([x, x**2]))\n",
    "logit_quad = sm.Logit(y, X_quadratic)\n",
    "logit_quad_result = logit_quad.fit(disp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likelihood ratio test: H₀: quadratic term coefficient = 0\n",
    "lr_statistic = logit_result.deviance - logit_quad_result.deviance\n",
    "df_difference = 1  # One additional parameter\n",
    "p_value_lr = 1 - stats.chi2.cdf(lr_statistic, df_difference)\n",
    "print(f\"Likelihood Ratio statistic: {lr_statistic:.3f}\")\n",
    "print(f\"p-value: {p_value_lr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.9 Model Diagnostics\n",
    "\n",
    "**Residuals for Generalized Linear Models:**\n",
    "\n",
    "**Deviance Residuals:** $r_i^D = \\text{sign}(y_i - \\hat{\\mu}_i)\\sqrt{d_i}$ where $d_i$ is contribution to deviance.\n",
    "\n",
    "**Pearson Residuals:** $r_i^P = \\frac{y_i - \\hat{\\mu}_i}{\\sqrt{\\widehat{\\text{Var}}(Y_i)}}$\n",
    "\n",
    "**Standardized Residuals:** Adjust for leverage: $r_i^{std} = \\frac{r_i}{\\sqrt{1-h_{ii}}}$\n",
    "\n",
    "**Diagnostic Plots:**\n",
    "- Residuals versus fitted values (check for patterns)\n",
    "- Quantile-Quantile plot (check distributional assumptions)\n",
    "- Cook's distance (identify influential observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute deviance residuals\n",
    "deviance_residuals = logit_result.resid_deviance\n",
    "fitted_values = logit_result.fittedvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plot: Check for patterns\n",
    "plt.scatter(fitted_values, deviance_residuals, alpha=0.6)\n",
    "plt.axhline(0, color='r', linestyle='--')\n",
    "plt.xlabel('Fitted Values'); plt.ylabel('Deviance Residuals')\n",
    "plt.title('Residual Plot for Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overdispersion\n",
    "\n",
    "**Problem:** For Poisson and binomial, variance is determined by mean. Real data often show greater variability.\n",
    "\n",
    "**Overdispersion:** $\\text{Var}(Y) > \\text{Var}_{model}(Y)$\n",
    "\n",
    "**Detection:** Residual deviance / degrees of freedom $\\gg 1$\n",
    "\n",
    "**Solutions:**\n",
    "- **Quasi-likelihood:** Estimate dispersion parameter from data\n",
    "- **Negative binomial:** For count data with overdispersion\n",
    "- **Beta-binomial:** For binomial data with overdispersion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for overdispersion in Poisson model\n",
    "X_count_with_intercept = sm.add_constant(x_count.reshape(-1, 1))\n",
    "poisson_sm = sm.GLM(y_count, X_count_with_intercept, family=sm.families.Poisson())\n",
    "poisson_sm_result = poisson_sm.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dispersion parameter estimate: deviance / df\n",
    "dispersion = poisson_sm_result.deviance / poisson_sm_result.df_resid\n",
    "print(f\"Dispersion parameter estimate: {dispersion:.3f}\")\n",
    "if dispersion > 1.5:\n",
    "    print(\"Evidence of overdispersion (should be ≈ 1 for Poisson)\")\n",
    "else:\n",
    "    print(\"No strong evidence of overdispersion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.10 Extensions and Related Models\n",
    "\n",
    "**Multinomial Logistic Regression:** Categorical response with > 2 levels\n",
    "\n",
    "**Ordinal Logistic Regression:** Ordered categorical response (e.g., ratings: poor/fair/good/excellent)\n",
    "\n",
    "**Negative Binomial Regression:** Count data with overdispersion\n",
    "\n",
    "**Zero-Inflated Models:** Count data with excess zeros\n",
    "\n",
    "**Generalized Additive Models:** Replace linear predictors with smooth functions: $g(\\mu_i) = f_1(x_{i1}) + f_2(x_{i2}) + ...$\n",
    "\n",
    "**Mixed Effects Models:** Include random effects for hierarchical/clustered data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Generalized Linear Models Framework\n",
    "\n",
    "**Generalized Linear Models unify diverse regression models:**\n",
    "\n",
    "**Three Components:**\n",
    "1. **Random component:** Response from exponential family (Normal, Binomial, Poisson, Gamma, etc.)\n",
    "2. **Systematic component:** Linear predictor $\\eta = \\mathbf{X}\\boldsymbol{\\beta}$\n",
    "3. **Link function:** Connects mean to linear predictor: $g(\\mu) = \\eta$\n",
    "\n",
    "**Key Models:**\n",
    "- **Linear regression:** Identity link, Normal distribution (continuous response)\n",
    "- **Logistic regression:** Logit link, Binomial distribution (binary response)\n",
    "- **Poisson regression:** Log link, Poisson distribution (count data)\n",
    "\n",
    "**Advantages:**\n",
    "- Unified framework for different response types\n",
    "- Link functions ensure predictions in valid range\n",
    "- Maximum Likelihood Estimation with good asymptotic properties\n",
    "- Flexible: can accommodate various distributions and relationships\n",
    "\n",
    "**Inference:**\n",
    "- Wald tests for individual coefficients\n",
    "- Likelihood ratio tests for nested models\n",
    "- Deviance measures goodness-of-fit\n",
    "- Residual analysis checks assumptions\n",
    "\n",
    "**Practical Considerations:**\n",
    "- Check for overdispersion (especially Poisson/binomial)\n",
    "- Use residual plots to diagnose problems\n",
    "- Compare nested models with likelihood ratio tests\n",
    "- Consider alternative link functions if canonical link fits poorly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- **Generalized Linear Models extend linear regression beyond normality:** By allowing exponential family distributions and using link functions, Generalized Linear Models handle binary, count, and other non-normal responses that ordinary linear regression cannot.\n",
    "\n",
    "- **Link functions ensure valid predictions:** Logit link keeps probabilities in [0,1], log link keeps counts positive. This prevents nonsensical predictions that arise from applying linear regression to constrained responses.\n",
    "\n",
    "- **Coefficients have multiplicative interpretations:** In logistic regression, $e^{\\beta}$ is an odds ratio. In Poisson regression, $e^{\\beta}$ is a rate ratio. This differs from linear regression's additive effects.\n",
    "\n",
    "- **Maximum Likelihood Estimation provides asymptotic normality:** Standard Errors, confidence intervals, and hypothesis tests follow from asymptotic theory, enabling inference even without exact sampling distributions.\n",
    "\n",
    "- **Deviance replaces residual sum of squares:** Deviance measures model fit for non-normal responses. Differences in deviance between nested models follow chi-squared distributions, enabling likelihood ratio tests.\n",
    "\n",
    "- **Residual analysis is still essential:** Deviance residuals and Pearson residuals help diagnose model problems, identify outliers, and check for overdispersion. Good Generalized Linear Model practice requires examining residuals just as in linear regression.\n",
    "\n",
    "- **Overdispersion is common in practice:** Real Poisson and binomial data often show more variability than the model assumes. Detecting and addressing overdispersion (via quasi-likelihood, negative binomial, etc.) is crucial for valid inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
