{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Introduction to Statistical Inference\n",
    "\n",
    "**Core Goal:** Use sample data to draw valid conclusions about populations.\n",
    "\n",
    "**Motivation:** In practice, we rarely have access to complete population data. Medical researchers cannot test drugs on every patient, pollsters cannot survey every voter, and manufacturers cannot inspect every product. Statistical inference provides principled methods to make reliable statements about populations based on limited sample data, while quantifying our uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Population versus Sample\n",
    "\n",
    "**Population:** The complete set of all individuals or items of interest for a particular study.\n",
    "\n",
    "**Sample:** A subset of the population that we actually observe and measure.\n",
    "\n",
    "**Motivation:** Collecting data from an entire population is typically impossible or impractical due to cost, time, or destructive testing. We must learn to draw conclusions about the whole from observing only a part. The fundamental challenge of statistical inference is to determine how much information a sample provides about its parent population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "population = stats.norm(loc=100, scale=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = population.rvs(size=30)\n",
    "print(f\"Sample (first 5 values): {sample[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insight:** We never observe the full population, only samples drawn from it. All inference involves reasoning from the known (sample) to the unknown (population)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Parameters versus Statistics\n",
    "\n",
    "**Parameter (θ):** A numerical characteristic of a population. Parameters are fixed but unknown constants.\n",
    "\n",
    "**Statistic:** A numerical characteristic computed from sample data. Statistics are random variables that vary from sample to sample.\n",
    "\n",
    "**Motivation:** Parameters describe what we want to know (population characteristics), while statistics describe what we can compute (sample summaries). The relationship between statistics and parameters is central to inference. We use statistics as estimators of parameters, and understanding how statistics behave across different samples allows us to quantify uncertainty in our estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_mu = 100  # Population mean (parameter)\n",
    "true_sigma = 15  # Population standard deviation (parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_mean = np.mean(sample)  # Statistic\n",
    "sample_std = np.std(sample, ddof=1)  # Statistic (unbiased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"True μ = {true_mu}, Estimated μ̂ = {sample_mean:.2f}\")\n",
    "print(f\"True σ = {true_sigma}, Estimated σ̂ = {sample_std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notation Convention:** $\\theta$ denotes a parameter, $\\hat{\\theta}$ denotes an estimator of that parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Random Variables\n",
    "\n",
    "**Random Variable:** A function that assigns a numerical value to each outcome of a random experiment.\n",
    "\n",
    "**Motivation:** Random variables provide the mathematical framework for modeling uncertainty. They allow us to move from describing specific outcomes to analyzing the pattern of all possible outcomes. In statistical inference, both the data we observe and the statistics we compute are realizations of random variables. Understanding their probabilistic behavior is essential for making valid inferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discrete Random Variable: Binomial (number of successes in n trials)\n",
    "X_discrete = stats.binom(n=10, p=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous Random Variable: Normal distribution\n",
    "X_continuous = stats.norm(loc=0, scale=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discrete Random Variable:** Takes countable values (0, 1, 2, ...).\n",
    "\n",
    "**Continuous Random Variable:** Takes any value in an interval or union of intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Probability Distributions\n",
    "\n",
    "**Probability Distribution:** A mathematical function that describes the likelihood of different outcomes for a random variable.\n",
    "\n",
    "**Motivation:** Distributions summarize all probabilistic information about a random variable. They tell us not just what values are possible, but how likely each value is. In inference, we assume data follow a particular distribution family, and our goal becomes estimating the parameters of that distribution. The choice of distribution shapes every subsequent inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Mass Function (Discrete)\n",
    "\n",
    "**Probability Mass Function:** For discrete random variable $X$, $P(X = k)$ gives probability at each point $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = np.arange(0, 11)\n",
    "pmf_values = X_discrete.pmf(k_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(k_values, pmf_values)\n",
    "plt.title('Binomial Probability Mass Function'); plt.xlabel('k'); plt.ylabel('P(X=k)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Density Function (Continuous)\n",
    "\n",
    "**Probability Density Function:** For continuous $X$, $f(x)$ is density at point $x$. Probability is area under curve: $P(a < X < b) = \\int_a^b f(x)dx$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = np.linspace(-4, 4, 100)\n",
    "pdf_values = X_continuous.pdf(x_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_values, pdf_values)\n",
    "plt.title('Normal Probability Density Function'); plt.xlabel('x'); plt.ylabel('f(x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative Distribution Function\n",
    "\n",
    "**Cumulative Distribution Function:** $F(x) = P(X \\leq x)$ gives probability of observing value at most $x$.\n",
    "\n",
    "**Motivation:** The cumulative distribution function provides cumulative probabilities and is useful for computing tail probabilities and quantiles, which appear throughout hypothesis testing and confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_values = X_continuous.cdf(x_values)\n",
    "plt.plot(x_values, cdf_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Normal Cumulative Distribution Function')\n",
    "plt.xlabel('x'); plt.ylabel('F(x) = P(X ≤ x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Expected Value and Variance\n",
    "\n",
    "**Expected Value:** $E[X] = \\mu$ is the long-run average value of random variable $X$.\n",
    "\n",
    "**Variance:** $\\text{Var}(X) = E[(X - \\mu)^2] = \\sigma^2$ measures spread of distribution around its mean.\n",
    "\n",
    "**Motivation:** Expected value and variance are the two most fundamental characteristics of any distribution. Expected value tells us the center (where the distribution is located), while variance tells us the spread (how dispersed values are). These two quantities alone fully characterize normal distributions, and they appear in virtually every statistical procedure. Understanding their properties is essential for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_binomial = X_discrete.mean()\n",
    "variance_binomial = X_discrete.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"E[X] = {mean_binomial:.2f}, Var(X) = {variance_binomial:.2f}\")\n",
    "print(f\"Binomial formulas: E[X] = np = {10*0.3}, Var(X) = np(1-p) = {10*0.3*0.7}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Deviation\n",
    "\n",
    "**Standard Deviation:** $\\sigma = \\sqrt{\\text{Var}(X)}$ is square root of variance, measured in same units as $X$.\n",
    "\n",
    "**Motivation:** While variance is mathematically convenient, its units are squared. Standard deviation returns to original units, making it more interpretable for describing spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_binomial = X_discrete.std()\n",
    "print(f\"Standard Deviation σ = {std_binomial:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Common Probability Distributions\n",
    "\n",
    "**Motivation:** Certain probability distributions appear repeatedly in statistical inference. Understanding their properties and relationships is crucial because different inference procedures depend on different distributional assumptions. These distributions form the theoretical foundation for most classical inference methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Distribution $N(\\mu, \\sigma^2)$\n",
    "\n",
    "**Normal Distribution:** Continuous distribution defined by two parameters: mean $\\mu$ and variance $\\sigma^2$.\n",
    "\n",
    "**Probability Density Function:** $f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$ for $-\\infty < x < \\infty$\n",
    "\n",
    "**Motivation:** The normal distribution is the most important distribution in statistics. Many natural phenomena are approximately normally distributed. More importantly, due to the Central Limit Theorem, sample means tend toward normality regardless of the original population distribution. This makes normal-based inference remarkably robust and widely applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_dist = stats.norm(loc=100, scale=15)\n",
    "print(f\"Parameters: μ = 100, σ = 15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 68-95-99.7 Rule: approximately 68% within 1σ, 95% within 2σ, 99.7% within 3σ\n",
    "print(f\"P(μ - σ < X < μ + σ) = {normal_dist.cdf(115) - normal_dist.cdf(85):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Normal Distribution $Z \\sim N(0,1)$\n",
    "\n",
    "**Standard Normal Distribution:** Normal distribution with mean 0 and variance 1.\n",
    "\n",
    "**Motivation:** Any normal random variable can be transformed to standard normal via standardization. This allows us to use a single standard normal table for all normal probability calculations. Critical values for hypothesis tests and confidence intervals are typically expressed in terms of standard normal quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_normal = stats.norm(0, 1)\n",
    "print(f\"P(Z < 1.96) = {standard_normal.cdf(1.96):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardization:** Any normal variable $X \\sim N(\\mu, \\sigma^2)$ can be transformed to standard normal: $Z = \\frac{X - \\mu}{\\sigma} \\sim N(0,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 130\n",
    "z_score = (x - 100) / 15; print(f\"Z-score for x=130: {z_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student's t-distribution\n",
    "\n",
    "**Student's t-distribution:** Distribution with heavier tails than normal, characterized by degrees of freedom parameter.\n",
    "\n",
    "**Motivation:** When estimating a population mean with unknown variance from a small sample, using the sample standard deviation introduces additional uncertainty. The t-distribution accounts for this extra variability. It is essential for small-sample inference about means. As sample size increases (degrees of freedom increase), the t-distribution approaches the normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_distribution = stats.t(df=10)\n",
    "x = np.linspace(-4, 4, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, standard_normal.pdf(x), label='Normal N(0,1)')\n",
    "plt.plot(x, t_distribution.pdf(x), label='t(df=10)'); plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('t-distribution versus Normal: Heavier Tails')\n",
    "plt.xlabel('x'); plt.ylabel('Density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Property:** As degrees of freedom $\\nu \\to \\infty$, $t_\\nu \\to N(0,1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-squared Distribution $\\chi^2$\n",
    "\n",
    "**Chi-squared Distribution:** Distribution of sum of squared independent standard normal variables.\n",
    "\n",
    "**Definition:** If $Z_1, ..., Z_\\nu$ are independent $N(0,1)$, then $\\chi^2 = \\sum_{i=1}^\\nu Z_i^2 \\sim \\chi^2_\\nu$\n",
    "\n",
    "**Motivation:** The chi-squared distribution arises naturally when dealing with variance estimation. Sample variance, when properly scaled, follows a chi-squared distribution. This makes it fundamental for inference about population variances and for goodness-of-fit tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_distribution = stats.chi2(df=5)\n",
    "x = np.linspace(0, 20, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, chi2_distribution.pdf(x))\n",
    "plt.title('Chi-squared Distribution (df=5)'); plt.xlabel('x'); plt.ylabel('Density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Primary Uses:** Variance estimation, goodness-of-fit tests, tests of independence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F-distribution\n",
    "\n",
    "**F-distribution:** Distribution of ratio of two independent chi-squared random variables, each divided by its degrees of freedom.\n",
    "\n",
    "**Definition:** If $\\chi^2_1 \\sim \\chi^2_{\\nu_1}$ and $\\chi^2_2 \\sim \\chi^2_{\\nu_2}$ are independent, then $F = \\frac{\\chi^2_1/\\nu_1}{\\chi^2_2/\\nu_2} \\sim F_{\\nu_1, \\nu_2}$\n",
    "\n",
    "**Motivation:** The F-distribution appears when comparing two variances or when partitioning variance into components. It is the foundation of Analysis of Variance (ANOVA) and regression F-tests, which compare explained versus unexplained variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_distribution = stats.f(dfn=5, dfd=10)\n",
    "x = np.linspace(0, 5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, f_distribution.pdf(x))\n",
    "plt.title('F-distribution F(5,10)'); plt.xlabel('x'); plt.ylabel('Density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Primary Uses:** Comparing variances, Analysis of Variance (ANOVA), regression model testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Sampling Distributions\n",
    "\n",
    "**Sampling Distribution:** The probability distribution of a statistic obtained from all possible samples of size $n$.\n",
    "\n",
    "**Motivation:** A statistic varies from sample to sample because samples vary. The sampling distribution describes this variation and quantifies the precision of our estimator. Understanding sampling distributions allows us to make probability statements about how close our estimate is likely to be to the true parameter. This is the bridge between sample statistics and population parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate: Draw many samples, compute mean of each\n",
    "sample_means = [population.rvs(30).mean() for _ in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sample_means, bins=30, density=True, alpha=0.7, edgecolor='black')\n",
    "plt.title('Sampling Distribution of Sample Mean X̄'); plt.xlabel('X̄'); plt.ylabel('Density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theoretical Result:** When sampling from $N(\\mu, \\sigma^2)$, the sample mean follows $\\bar{X} \\sim N(\\mu, \\sigma^2/n)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"E[X̄] - Theory: {true_mu}, Empirical: {np.mean(sample_means):.2f}\")\n",
    "print(f\"Standard Error - Theory: {true_sigma/np.sqrt(30):.2f}, Empirical: {np.std(sample_means):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Central Limit Theorem\n",
    "\n",
    "**Central Limit Theorem:** For large sample size $n$, the distribution of $\\bar{X}$ is approximately normal regardless of the population distribution.\n",
    "\n",
    "**Formal Statement:** If $X_1, ..., X_n$ are independent and identically distributed with mean $\\mu$ and variance $\\sigma^2$, then $\\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\xrightarrow{d} N(0,1)$ as $n \\to \\infty$\n",
    "\n",
    "**Motivation:** The Central Limit Theorem is arguably the most important result in statistics. It explains why normal distributions appear everywhere in practice and why normal-based inference works even when data are not normal. It provides theoretical justification for using normal approximations with large samples, making inference possible without knowing the exact population distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-normal population: Exponential (highly skewed)\n",
    "exponential_population = stats.expon(scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample means from exponential population\n",
    "exponential_means = [exponential_population.rvs(30).mean() for _ in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(exponential_means, bins=30, density=True, edgecolor='black')\n",
    "plt.title('Sample Mean from Exponential Population: Still Approximately Normal!'); plt.xlabel('X̄')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practical Implication:** Normal theory applies broadly to sample means, even when individual observations are non-normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9 Three Types of Statistical Inference\n",
    "\n",
    "**Motivation:** Statistical inference answers three fundamental questions about parameters: What is a good single estimate? What range of values is plausible? Is a specific hypothesized value consistent with the data? These correspond to point estimation, interval estimation, and hypothesis testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9.1 Point Estimation\n",
    "\n",
    "**Point Estimation:** Using sample data to calculate a single number as an estimate of an unknown parameter.\n",
    "\n",
    "**Motivation:** Point estimates provide our \"best guess\" for a parameter value. While they do not convey uncertainty, they are often required for decision-making and form the basis for more sophisticated inference procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = stats.norm(100, 15).rvs(50)\n",
    "mu_hat = np.mean(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Point estimate for μ: μ̂ = {mu_hat:.2f}\")\n",
    "print(\"This is our single best guess for the population mean.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9.2 Interval Estimation (Confidence Intervals)\n",
    "\n",
    "**Confidence Interval:** A range of plausible values for a parameter, constructed so that it captures the true parameter value with specified probability (confidence level).\n",
    "\n",
    "**Motivation:** A point estimate alone provides no information about precision. A confidence interval quantifies uncertainty by giving a range of plausible parameter values. The confidence level (typically 95%) refers to the long-run proportion of such intervals that contain the true parameter if we repeated the sampling process many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_error = stats.sem(data)\n",
    "confidence_interval = stats.t.interval(0.95, len(data)-1, loc=mu_hat, scale=standard_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"95% Confidence Interval for μ: [{confidence_interval[0]:.2f}, {confidence_interval[1]:.2f}]\")\n",
    "print(\"We are 95% confident this interval contains the true population mean.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correct Interpretation:** If we repeated this procedure many times with different samples, approximately 95% of the resulting intervals would contain the true parameter value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9.3 Hypothesis Testing\n",
    "\n",
    "**Hypothesis Testing:** A formal procedure for assessing whether sample data provide sufficient evidence to reject a specific claim about a parameter.\n",
    "\n",
    "**Motivation:** Often we want to determine whether data support or contradict a specific hypothesis. For example, does a drug work better than placebo? Is a coin fair? Hypothesis testing provides a principled framework for making such decisions while controlling the probability of false conclusions. The p-value quantifies how surprising the observed data would be if the hypothesis were true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test null hypothesis H₀: μ = 105 versus alternative H₁: μ ≠ 105\n",
    "test_result = stats.ttest_1samp(data, 105)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test statistic: t = {test_result.statistic:.3f}\")\n",
    "print(f\"P-value: {test_result.pvalue:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Rule:** Reject null hypothesis H₀ if p-value < significance level α (typically α = 0.05).\n",
    "\n",
    "**P-value Interpretation:** Probability of observing a test statistic at least as extreme as ours, assuming H₀ is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision = \"Reject H₀\" if test_result.pvalue < 0.05 else \"Fail to reject H₀\"\n",
    "print(f\"Decision at α = 0.05: {decision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.10 Standard Error versus Standard Deviation\n",
    "\n",
    "**Standard Deviation:** Measures variability of individual observations in a population or sample: $\\sigma = \\sqrt{\\text{Var}(X)}$\n",
    "\n",
    "**Standard Error:** Measures variability of a sample statistic across different samples: $\\text{SE}(\\bar{X}) = \\frac{\\sigma}{\\sqrt{n}}$\n",
    "\n",
    "**Motivation:** These two concepts are often confused but measure fundamentally different things. Standard deviation describes spread in the data itself. Standard error describes precision of an estimator. As sample size increases, standard deviation stays roughly constant (it estimates a fixed population parameter), but standard error decreases (our estimator becomes more precise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sd = np.std(data, ddof=1)\n",
    "print(f\"Standard Deviation of observations: {sample_sd:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_se = sample_sd / np.sqrt(len(data))\n",
    "print(f\"Standard Error of sample mean: {sample_se:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Formula:** $\\text{SE}(\\bar{X}) = \\frac{\\sigma}{\\sqrt{n}}$. Standard error decreases as sample size increases, reflecting increased precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.11 Bias and Variance of Estimators\n",
    "\n",
    "**Bias:** The systematic error in an estimator: $\\text{Bias}(\\hat{\\theta}) = E[\\hat{\\theta}] - \\theta$\n",
    "\n",
    "**Unbiased Estimator:** An estimator with zero bias: $E[\\hat{\\theta}] = \\theta$\n",
    "\n",
    "**Motivation:** An estimator's quality depends on two factors: whether it hits the target on average (bias) and how much it varies (variance). Unbiased estimators are neither systematically too high nor too low. However, unbiasedness alone does not guarantee a good estimator; precision (low variance) is equally important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate sample mean is unbiased for population mean\n",
    "estimates = [population.rvs(30).mean() for _ in range(5000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = np.mean(estimates) - true_mu\n",
    "print(f\"Bias of sample mean: {bias:.4f} (approximately 0, confirming unbiasedness)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variance of Estimator:** $\\text{Var}(\\hat{\\theta})$ measures how much the estimator varies across different samples. Lower variance means more precise estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empirical_variance = np.var(estimates)\n",
    "theoretical_variance = true_sigma**2 / 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Empirical Var(X̄) = {empirical_variance:.2f}\")\n",
    "print(f\"Theoretical Var(X̄) = σ²/n = {theoretical_variance:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.12 Mean Squared Error\n",
    "\n",
    "**Mean Squared Error:** Overall measure of estimator quality: $\\text{MSE}(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2] = \\text{Bias}^2(\\hat{\\theta}) + \\text{Var}(\\hat{\\theta})$\n",
    "\n",
    "**Motivation:** Mean squared error combines both bias and variance into a single measure of estimation accuracy. It represents the expected squared distance between estimator and true parameter. The decomposition into bias² and variance reveals that both systematic error and random variation contribute to total error. Sometimes accepting small bias yields lower variance and better overall mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = np.mean((np.array(estimates) - true_mu)**2)\n",
    "print(f\"Mean Squared Error of sample mean: {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For unbiased estimator: MSE equals Variance\n",
    "print(f\"MSE ≈ Variance: {np.isclose(mse, empirical_variance)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.13 Law of Large Numbers\n",
    "\n",
    "**Law of Large Numbers:** As sample size $n \\to \\infty$, the sample mean $\\bar{X}$ converges in probability to the population mean $\\mu$.\n",
    "\n",
    "**Formal Statement:** $\\bar{X}_n \\xrightarrow{P} \\mu$ as $n \\to \\infty$\n",
    "\n",
    "**Motivation:** The Law of Large Numbers provides the theoretical justification for using sample averages to estimate population means. It guarantees that with enough data, our estimate will be arbitrarily close to the true value with high probability. This fundamental result underpins the entire practice of using samples to learn about populations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes = [10, 50, 100, 500, 1000, 5000]\n",
    "means_by_size = [population.rvs(n).mean() for n in sample_sizes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sample_sizes, means_by_size, 'o-', markersize=8)\n",
    "plt.axhline(true_mu, color='r', linestyle='--', linewidth=2, label='True μ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Sample Size n'); plt.ylabel('Sample Mean X̄')\n",
    "plt.title('Law of Large Numbers: X̄ Converges to μ'); plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: The Statistical Inference Framework\n",
    "\n",
    "1. **Population** contains unknown parameter $\\theta$ that we want to learn about\n",
    "2. Draw **random sample** $X_1, X_2, ..., X_n$ from the population\n",
    "3. Compute **statistic** $\\hat{\\theta}$ (estimator) from the sample\n",
    "4. Use **sampling distribution** of $\\hat{\\theta}$ to quantify uncertainty\n",
    "5. Perform inference using one of three methods:\n",
    "   - **Point estimation:** Single best estimate\n",
    "   - **Interval estimation:** Range of plausible values with confidence level\n",
    "   - **Hypothesis testing:** Assess evidence for or against specific claim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- **Randomness is fundamental:** Samples vary randomly, causing statistics to vary. Understanding this variation is essential for inference.\n",
    "\n",
    "- **Sampling distributions quantify uncertainty:** The distribution of a statistic across all possible samples tells us how precise our estimates are.\n",
    "\n",
    "- **Central Limit Theorem enables robust inference:** Sample means are approximately normal for large samples, regardless of population distribution, making normal-based methods widely applicable.\n",
    "\n",
    "- **Good estimators balance bias and variance:** Unbiased estimators hit the target on average, but low variance (high precision) is equally important. Mean squared error captures both.\n",
    "\n",
    "- **Standard error quantifies precision:** Standard error measures how much an estimator varies across samples. It decreases with $\\sqrt{n}$, so larger samples give more precise estimates.\n",
    "\n",
    "- **Three types of inference address different questions:** Point estimates provide best guesses, confidence intervals quantify uncertainty, and hypothesis tests assess specific claims."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
