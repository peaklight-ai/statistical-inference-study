{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Maximum Likelihood Estimation\n",
    "\n",
    "**Core Goal:** Find parameter values that make observed data most probable.\n",
    "\n",
    "**Motivation:** Given sample data and a parametric model, which parameter value best explains what we observed? Maximum Likelihood Estimation answers this by choosing the parameter that maximizes the probability (likelihood) of the observed data. This principle is intuitive, theoretically justified, and produces estimators with excellent properties. Maximum Likelihood Estimators are consistent, asymptotically normal, and asymptotically efficient, making them a cornerstone of statistical inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Likelihood Function\n",
    "\n",
    "**Likelihood Function:** $L(\\theta; x) = f(x; \\theta)$ viewed as a function of parameter $\\theta$ for fixed data $x$.\n",
    "\n",
    "**For independent observations:** $L(\\theta; x_1, ..., x_n) = \\prod_{i=1}^n f(x_i; \\theta)$\n",
    "\n",
    "**Motivation:** The likelihood function represents how probable the observed data are as a function of different parameter values. While probability fixes the parameter and varies the data, likelihood fixes the data and varies the parameter. This perspective shift is crucial: we ask which parameter values make our observed data more or less likely. Higher likelihood means the parameter value provides a better explanation for the data we actually observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "true_mu = 5; data = stats.norm(true_mu, 1).rvs(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L(μ) = ∏f(xᵢ; μ): Product of densities at observed data points\n",
    "mu_values = np.linspace(0, 10, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute likelihood for each candidate parameter value\n",
    "likelihoods = [np.prod(stats.norm(mu, 1).pdf(data)) for mu in mu_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mu_values, likelihoods, linewidth=2)\n",
    "plt.axvline(true_mu, color='r', linestyle='--', label=f'True μ = {true_mu}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Parameter μ'); plt.ylabel('Likelihood L(μ)')\n",
    "plt.title('Likelihood Function for Normal Mean'); plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insight:** Likelihood peaks near the true parameter value, showing which $\\mu$ makes observed data most probable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Log-Likelihood Function\n",
    "\n",
    "**Log-Likelihood:** $\\ell(\\theta; x) = \\log L(\\theta; x) = \\sum_{i=1}^n \\log f(x_i; \\theta)$\n",
    "\n",
    "**Motivation:** Products are computationally unstable and analytically unwieldy. Taking logarithms converts products to sums, which are easier to compute and differentiate. Since logarithm is monotonically increasing, maximizing log-likelihood is equivalent to maximizing likelihood. The log transformation prevents numerical underflow (likelihoods can be extremely small) and simplifies derivative calculations needed for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ℓ(μ) = Σlog f(xᵢ; μ): Sum of log-densities for numerical stability\n",
    "log_likelihoods = [np.sum(stats.norm(mu, 1).logpdf(data)) for mu in mu_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mu_values, log_likelihoods, linewidth=2)\n",
    "plt.axvline(true_mu, color='r', linestyle='--', label=f'True μ = {true_mu}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Parameter μ'); plt.ylabel('Log-Likelihood ℓ(μ)')\n",
    "plt.title('Log-Likelihood Function (Same Maximum as Likelihood)'); plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computational Advantage:** Log-likelihood avoids underflow and is much more stable for numerical optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Maximum Likelihood Estimator\n",
    "\n",
    "**Maximum Likelihood Estimator:** $\\hat{\\theta}_{MLE} = \\arg\\max_\\theta L(\\theta; x) = \\arg\\max_\\theta \\ell(\\theta; x)$\n",
    "\n",
    "**Motivation:** The Maximum Likelihood Estimator chooses the parameter value that makes the observed data most probable under the assumed model. This is an intuitive principle: among all possible parameter values, select the one under which what we actually observed would be most likely to occur. Maximum Likelihood Estimation provides a unified framework that works for virtually any parametric model, and the resulting estimators have strong theoretical properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# μ̂ₘₗₑ = arg max L(μ): Parameter value maximizing likelihood\n",
    "mu_hat_mle = mu_values[np.argmax(log_likelihoods)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Maximum Likelihood Estimator: μ̂ₘₗₑ = {mu_hat_mle:.3f}\")\n",
    "print(f\"Sample mean: X̄ = {np.mean(data):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:** For normal distribution, Maximum Likelihood Estimator equals sample mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Finding Maximum Likelihood Estimator Analytically\n",
    "\n",
    "**Analytic Method:** Solve $\\frac{\\partial \\ell(\\theta)}{\\partial \\theta} = 0$ (score equation)\n",
    "\n",
    "**Motivation:** When the log-likelihood is differentiable and concave, we can find the Maximum Likelihood Estimator by setting the derivative to zero. This is often simpler than numerical optimization and provides closed-form solutions that reveal the estimator's structure. The derivative of log-likelihood is called the score function, and it plays a central role in likelihood theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Normal Mean with Known Variance\n",
    "\n",
    "**Model:** $X_1, ..., X_n \\sim N(\\mu, \\sigma^2)$ with $\\sigma^2$ known\n",
    "\n",
    "**Log-likelihood:** $\\ell(\\mu) = -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i - \\mu)^2$\n",
    "\n",
    "**Score function:** $\\frac{\\partial \\ell}{\\partial \\mu} = \\frac{1}{\\sigma^2}\\sum_{i=1}^n(x_i - \\mu)$\n",
    "\n",
    "**Setting to zero:** $\\sum_{i=1}^n(x_i - \\mu) = 0 \\implies \\hat{\\mu}_{MLE} = \\frac{1}{n}\\sum_{i=1}^n x_i = \\bar{X}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# μ̂ₘₗₑ = X̄: Sample mean is Maximum Likelihood Estimator for normal mean\n",
    "mu_hat_analytic = np.mean(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Analytic Maximum Likelihood Estimator: {mu_hat_analytic:.3f}\")\n",
    "print(\"Matches numerical optimization result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Normal Variance with Known Mean\n",
    "\n",
    "**Model:** $X_1, ..., X_n \\sim N(\\mu, \\sigma^2)$ with $\\mu$ known\n",
    "\n",
    "**Log-likelihood:** $\\ell(\\sigma^2) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i - \\mu)^2$\n",
    "\n",
    "**Score:** $\\frac{\\partial \\ell}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2}\\sum_{i=1}^n(x_i - \\mu)^2$\n",
    "\n",
    "**Maximum Likelihood Estimator:** $\\hat{\\sigma}^2_{MLE} = \\frac{1}{n}\\sum_{i=1}^n(x_i - \\mu)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# σ̂²ₘₗₑ = (1/n)Σ(xᵢ - μ)²: Average squared deviation from known mean\n",
    "sigma_sq_hat = np.mean((data - true_mu)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Maximum Likelihood Estimator for σ²: {sigma_sq_hat:.3f}\")\n",
    "print(f\"True σ² = 1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Both Parameters Unknown\n",
    "\n",
    "**Joint Maximum Likelihood Estimators:**\n",
    "- $\\hat{\\mu}_{MLE} = \\bar{X}$\n",
    "- $\\hat{\\sigma}^2_{MLE} = \\frac{1}{n}\\sum_{i=1}^n(X_i - \\bar{X})^2$\n",
    "\n",
    "**Note:** $\\hat{\\sigma}^2_{MLE}$ divides by $n$, making it biased. The unbiased estimator divides by $n-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# σ̂²ₘₗₑ = (1/n)Σ(xᵢ - X̄)²: Biased Maximum Likelihood Estimator (divides by n)\n",
    "sigma_sq_mle_biased = np.mean((data - np.mean(data))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s² = (1/(n-1))Σ(xᵢ - X̄)²: Unbiased estimator (divides by n-1)\n",
    "sigma_sq_unbiased = np.var(data, ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Maximum Likelihood Estimator (biased): {sigma_sq_mle_biased:.3f}\")\n",
    "print(f\"Unbiased estimator: {sigma_sq_unbiased:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Maximum Likelihood Estimation for Bernoulli Distribution\n",
    "\n",
    "**Model:** $X_1, ..., X_n \\sim \\text{Bernoulli}(p)$\n",
    "\n",
    "**Probability Mass Function:** $P(X = x) = p^x(1-p)^{1-x}$ for $x \\in \\{0, 1\\}$\n",
    "\n",
    "**Likelihood:** $L(p) = \\prod_{i=1}^n p^{x_i}(1-p)^{1-x_i} = p^{\\sum x_i}(1-p)^{n - \\sum x_i}$\n",
    "\n",
    "**Log-likelihood:** $\\ell(p) = \\left(\\sum x_i\\right) \\log p + \\left(n - \\sum x_i\\right) \\log(1-p)$\n",
    "\n",
    "**Maximum Likelihood Estimator:** $\\hat{p}_{MLE} = \\frac{1}{n}\\sum_{i=1}^n X_i = \\bar{X}$\n",
    "\n",
    "**Motivation:** For binary data, Maximum Likelihood Estimation yields the sample proportion of successes, which is the natural estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_p = 0.3\n",
    "bernoulli_data = stats.bernoulli(true_p).rvs(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p̂ₘₗₑ = X̄: Sample proportion is Maximum Likelihood Estimator for Bernoulli parameter\n",
    "p_hat_mle = np.mean(bernoulli_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"True p = {true_p}\")\n",
    "print(f\"Maximum Likelihood Estimator: p̂ₘₗₑ = {p_hat_mle:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Maximum Likelihood Estimation for Exponential Distribution\n",
    "\n",
    "**Model:** $X_1, ..., X_n \\sim \\text{Exp}(\\lambda)$\n",
    "\n",
    "**Probability Density Function:** $f(x; \\lambda) = \\lambda e^{-\\lambda x}$ for $x > 0$\n",
    "\n",
    "**Log-likelihood:** $\\ell(\\lambda) = n\\log\\lambda - \\lambda\\sum_{i=1}^n x_i$\n",
    "\n",
    "**Score:** $\\frac{\\partial \\ell}{\\partial \\lambda} = \\frac{n}{\\lambda} - \\sum_{i=1}^n x_i$\n",
    "\n",
    "**Maximum Likelihood Estimator:** $\\hat{\\lambda}_{MLE} = \\frac{n}{\\sum_{i=1}^n x_i} = \\frac{1}{\\bar{X}}$\n",
    "\n",
    "**Motivation:** For exponential data (waiting times, lifetimes), Maximum Likelihood Estimation gives the reciprocal of the sample mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_lambda = 2\n",
    "exp_data = stats.expon(scale=1/true_lambda).rvs(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# λ̂ₘₗₑ = 1/X̄: Reciprocal of sample mean for exponential rate\n",
    "lambda_hat_mle = 1 / np.mean(exp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"True λ = {true_lambda}\")\n",
    "print(f\"Maximum Likelihood Estimator: λ̂ₘₗₑ = {lambda_hat_mle:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Numerical Maximum Likelihood Estimation\n",
    "\n",
    "**Motivation:** Many models lack closed-form Maximum Likelihood Estimators. Numerical optimization finds the maximum by iterative algorithms. This approach works for arbitrarily complex models, though it requires careful implementation to avoid local maxima and convergence issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data from normal distribution\n",
    "data_normal = stats.norm(10, 2).rvs(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -ℓ(θ): Negative log-likelihood (minimize instead of maximize)\n",
    "def neg_log_likelihood(params):\n",
    "    return -np.sum(stats.norm(params[0], params[1]).logpdf(data_normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical optimization to find Maximum Likelihood Estimators\n",
    "result = minimize(neg_log_likelihood, x0=[0, 1], method='L-BFGS-B', bounds=[(None, None), (0.001, None)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Numerical Maximum Likelihood Estimators: μ̂ = {result.x[0]:.3f}, σ̂ = {result.x[1]:.3f}\")\n",
    "print(f\"Analytic: μ̂ = {np.mean(data_normal):.3f}, σ̂ = {np.std(data_normal, ddof=0):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Invariance Property of Maximum Likelihood Estimator\n",
    "\n",
    "**Invariance Property:** If $\\hat{\\theta}_{MLE}$ is the Maximum Likelihood Estimator of $\\theta$, then $g(\\hat{\\theta}_{MLE})$ is the Maximum Likelihood Estimator of $g(\\theta)$ for any function $g$.\n",
    "\n",
    "**Motivation:** This remarkable property simplifies estimation of transformed parameters. If we want to estimate a function of a parameter, we simply apply that function to the Maximum Likelihood Estimator of the original parameter. No separate optimization is needed. This property is unique to Maximum Likelihood Estimation and does not hold for other estimation methods like Method of Moments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For normal data: μ̂ₘₗₑ = X̄\n",
    "mu_hat = np.mean(data_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g(μ̂ₘₗₑ): Maximum Likelihood Estimator of g(μ) by invariance property\n",
    "tau_hat = np.exp(mu_hat)  # Estimate e^μ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"μ̂ₘₗₑ = {mu_hat:.3f}\")\n",
    "print(f\"Maximum Likelihood Estimator of e^μ: e^μ̂ = {tau_hat:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** If $\\hat{\\lambda}_{MLE}$ for exponential, then Maximum Likelihood Estimator of mean $1/\\lambda$ is $1/\\hat{\\lambda}_{MLE}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9 Score Function\n",
    "\n",
    "**Score Function:** $S(\\theta) = \\frac{\\partial \\ell(\\theta)}{\\partial \\theta}$\n",
    "\n",
    "**Properties:**\n",
    "1. $E[S(\\theta)] = 0$ (expected score is zero at true parameter)\n",
    "2. At Maximum Likelihood Estimator: $S(\\hat{\\theta}_{MLE}) = 0$\n",
    "\n",
    "**Motivation:** The score function measures the slope of log-likelihood. Its expectation being zero means that on average, the log-likelihood has no upward or downward trend at the true parameter. The score plays a central role in maximum likelihood theory and appears in asymptotic distribution results and efficiency calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S(μ) = ∂ℓ/∂μ: Derivative of log-likelihood (score function)\n",
    "def score_normal_mean(mu, data, sigma=1):\n",
    "    return np.sum((data - mu)) / sigma**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_grid = np.linspace(3, 7, 100)\n",
    "scores = [score_normal_mean(mu, data) for mu in mu_grid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mu_grid, scores, linewidth=2)\n",
    "plt.axhline(0, color='black', linestyle='-', linewidth=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Parameter μ'); plt.ylabel('Score S(μ)')\n",
    "plt.title('Score Function: Zero at Maximum Likelihood Estimator')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** Score crosses zero exactly where likelihood is maximized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.10 Fisher Information\n",
    "\n",
    "**Fisher Information:** $I(\\theta) = E\\left[\\left(\\frac{\\partial \\log f(X;\\theta)}{\\partial \\theta}\\right)^2\\right] = -E\\left[\\frac{\\partial^2 \\log f(X;\\theta)}{\\partial \\theta^2}\\right]$\n",
    "\n",
    "**For sample of size n:** $I_n(\\theta) = n I(\\theta)$\n",
    "\n",
    "**Motivation:** Fisher Information quantifies how much information the data contain about the parameter. Higher information means the log-likelihood is more sharply peaked, allowing more precise estimation. Fisher Information appears in the Cramér-Rao Lower Bound and in the asymptotic variance of Maximum Likelihood Estimators. It connects the curvature of log-likelihood to estimation precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Normal Distribution with Known Variance\n",
    "\n",
    "**Log-likelihood for one observation:** $\\ell(\\mu; x) = -\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{(x-\\mu)^2}{2\\sigma^2}$\n",
    "\n",
    "**First derivative:** $\\frac{\\partial \\ell}{\\partial \\mu} = \\frac{x - \\mu}{\\sigma^2}$\n",
    "\n",
    "**Second derivative:** $\\frac{\\partial^2 \\ell}{\\partial \\mu^2} = -\\frac{1}{\\sigma^2}$\n",
    "\n",
    "**Fisher Information:** $I(\\mu) = \\frac{1}{\\sigma^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I(μ) = 1/σ²: Fisher Information for normal mean\n",
    "sigma = 2; fisher_info = 1 / sigma**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Fisher Information for one observation: I(μ) = {fisher_info:.3f}\")\n",
    "print(f\"Fisher Information for n=100: I₁₀₀(μ) = {100 * fisher_info:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:** Information increases with sample size and decreases with variance. More observations or less noise mean more information about $\\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.11 Asymptotic Properties of Maximum Likelihood Estimator\n",
    "\n",
    "**Under regularity conditions, Maximum Likelihood Estimators have three key asymptotic properties:**\n",
    "\n",
    "1. **Consistency:** $\\hat{\\theta}_{MLE} \\xrightarrow{P} \\theta$ as $n \\to \\infty$\n",
    "2. **Asymptotic Normality:** $\\sqrt{n}(\\hat{\\theta}_{MLE} - \\theta) \\xrightarrow{d} N(0, 1/I(\\theta))$\n",
    "3. **Asymptotic Efficiency:** Maximum Likelihood Estimator achieves Cramér-Rao Lower Bound asymptotically\n",
    "\n",
    "**Motivation:** These properties explain why Maximum Likelihood Estimation is preferred in practice. Consistency guarantees convergence to truth with enough data. Asymptotic normality enables construction of confidence intervals and hypothesis tests using normal theory. Asymptotic efficiency means no other estimator can have lower asymptotic variance. Together, these properties make Maximum Likelihood Estimators the gold standard for large-sample inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrating Consistency\n",
    "\n",
    "**Consistency:** As sample size increases, Maximum Likelihood Estimator converges to true parameter value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_theta = 5\n",
    "sample_sizes = [10, 30, 100, 300, 1000, 3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Maximum Likelihood Estimators for increasing sample sizes\n",
    "mles = [stats.norm(true_theta, 2).rvs(n).mean() for n in sample_sizes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sample_sizes, mles, 'o-', markersize=8, linewidth=2)\n",
    "plt.axhline(true_theta, color='r', linestyle='--', linewidth=2, label='True θ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Sample Size n'); plt.ylabel('θ̂ₘₗₑ')\n",
    "plt.title('Consistency: Maximum Likelihood Estimator Converges to True Value'); plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrating Asymptotic Normality\n",
    "\n",
    "**Asymptotic Normality:** Sampling distribution of Maximum Likelihood Estimator becomes approximately normal for large $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sampling distribution of Maximum Likelihood Estimator\n",
    "mle_estimates = [stats.norm(true_theta, 2).rvs(100).mean() for _ in range(2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Var(θ̂ₘₗₑ) ≈ 1/(nI(θ)): Asymptotic variance formula\n",
    "asymptotic_var = 1 / (100 * fisher_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mle_estimates, bins=40, density=True, alpha=0.7, edgecolor='black')\n",
    "x = np.linspace(4, 6, 100); plt.plot(x, stats.norm(true_theta, np.sqrt(asymptotic_var)).pdf(x), 'r-', linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('θ̂ₘₗₑ'); plt.ylabel('Density')\n",
    "plt.title('Asymptotic Normality of Maximum Likelihood Estimator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Empirical standard deviation: {np.std(mle_estimates):.3f}\")\n",
    "print(f\"Theoretical (1/√(nI(θ))): {np.sqrt(asymptotic_var):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.12 Observed Information and Standard Errors\n",
    "\n",
    "**Observed Information:** $J(\\hat{\\theta}) = -\\frac{\\partial^2 \\ell(\\theta)}{\\partial \\theta^2}\\Big|_{\\theta = \\hat{\\theta}}$\n",
    "\n",
    "**Estimated Standard Error:** $\\widehat{SE}(\\hat{\\theta}_{MLE}) = \\frac{1}{\\sqrt{J(\\hat{\\theta})}}$\n",
    "\n",
    "**Motivation:** Observed information uses the actual data to estimate Fisher Information. The negative second derivative of log-likelihood at the Maximum Likelihood Estimator quantifies how peaked the likelihood is. A sharper peak means more information and smaller standard error. This provides a practical way to quantify uncertainty in Maximum Likelihood Estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# J(μ̂) = n/σ²: Observed information for normal mean\n",
    "observed_info = len(data) / sigma**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SE(μ̂ₘₗₑ) = 1/√J(μ̂): Standard error from observed information\n",
    "se_mle = 1 / np.sqrt(observed_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Observed Information: J(μ̂) = {observed_info:.2f}\")\n",
    "print(f\"Standard Error of Maximum Likelihood Estimator: {se_mle:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.13 Multiparameter Maximum Likelihood Estimation\n",
    "\n",
    "**Vector Parameter:** $\\theta = (\\theta_1, ..., \\theta_p)$\n",
    "\n",
    "**Score Vector:** $S(\\theta) = \\left(\\frac{\\partial \\ell}{\\partial \\theta_1}, ..., \\frac{\\partial \\ell}{\\partial \\theta_p}\\right)$\n",
    "\n",
    "**Maximum Likelihood Estimator:** Solve $S(\\hat{\\theta}) = 0$\n",
    "\n",
    "**Fisher Information Matrix:** $I(\\theta) = E\\left[S(\\theta)S(\\theta)^T\\right]$\n",
    "\n",
    "**Asymptotic Distribution:** $\\hat{\\theta}_{MLE} \\sim N(\\theta, I_n^{-1}(\\theta))$ for large $n$\n",
    "\n",
    "**Motivation:** Many realistic models have multiple parameters. The multiparameter Maximum Likelihood framework extends naturally to vector parameters. The Fisher Information becomes a matrix capturing information about each parameter and their relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal(μ, σ²): Two-parameter Maximum Likelihood Estimation\n",
    "data_multi = stats.norm(10, 3).rvs(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# μ̂ₘₗₑ = X̄: Maximum Likelihood Estimator for mean\n",
    "mu_hat_multi = np.mean(data_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# σ̂²ₘₗₑ = (1/n)Σ(xᵢ - X̄)²: Maximum Likelihood Estimator for variance\n",
    "sigma_sq_hat_multi = np.mean((data_multi - mu_hat_multi)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Maximum Likelihood Estimators: μ̂ = {mu_hat_multi:.3f}, σ̂² = {sigma_sq_hat_multi:.3f}\")\n",
    "print(f\"True values: μ = 10, σ² = 9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.14 Advantages and Limitations of Maximum Likelihood Estimation\n",
    "\n",
    "**Advantages:**\n",
    "1. **Principled:** Clear interpretation - parameter making data most probable\n",
    "2. **General:** Applicable to virtually any parametric model\n",
    "3. **Efficient:** Achieves Cramér-Rao Lower Bound asymptotically\n",
    "4. **Invariant:** Invariance property simplifies transformed parameters\n",
    "5. **Normal:** Asymptotically normal, enabling standard inference\n",
    "\n",
    "**Limitations:**\n",
    "1. **Model dependence:** Requires correctly specified probability model\n",
    "2. **Finite-sample bias:** May be biased in small samples\n",
    "3. **Computational:** May require numerical optimization\n",
    "4. **Regularity conditions:** Asymptotic theory requires technical assumptions\n",
    "5. **Not robust:** Sensitive to outliers and model misspecification\n",
    "\n",
    "**Motivation:** Understanding both strengths and weaknesses guides appropriate use of Maximum Likelihood Estimation. It excels with large samples and correctly specified models but may struggle with small samples or model violations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Maximum Likelihood Estimation Framework\n",
    "\n",
    "1. **Specify parametric model** $f(x; \\theta)$ for the data\n",
    "2. **Construct likelihood** $L(\\theta) = \\prod_{i=1}^n f(x_i; \\theta)$ or log-likelihood $\\ell(\\theta) = \\sum_{i=1}^n \\log f(x_i; \\theta)$\n",
    "3. **Maximize likelihood** by solving $\\frac{\\partial \\ell}{\\partial \\theta} = 0$ or using numerical optimization\n",
    "4. **Verify maximum** by checking second derivative is negative (concave)\n",
    "5. **Quantify uncertainty** using observed information $J(\\hat{\\theta})$ and standard errors\n",
    "6. **Use asymptotic theory** for large-sample inference: $\\hat{\\theta}_{MLE} \\sim N(\\theta, 1/(nI(\\theta)))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- **Likelihood quantifies plausibility:** Likelihood function shows which parameter values make observed data more or less probable. Maximizing likelihood chooses the most plausible parameter.\n",
    "\n",
    "- **Log-likelihood simplifies computation:** Taking logarithms converts products to sums, preventing numerical underflow and simplifying derivatives while preserving the maximum.\n",
    "\n",
    "- **Maximum Likelihood Estimators have excellent properties:** Consistency, asymptotic normality, and asymptotic efficiency make Maximum Likelihood Estimation the preferred method for large samples.\n",
    "\n",
    "- **Invariance property is powerful:** Estimating functions of parameters requires only applying the function to Maximum Likelihood Estimators, no separate optimization needed.\n",
    "\n",
    "- **Fisher Information quantifies precision:** Higher information means sharper likelihood peak and more precise estimation. Information increases with sample size and data quality.\n",
    "\n",
    "- **Observed information provides standard errors:** Negative second derivative of log-likelihood at Maximum Likelihood Estimator gives practical uncertainty quantification.\n",
    "\n",
    "- **Numerical methods handle complex models:** When no closed-form solution exists, numerical optimization finds Maximum Likelihood Estimators, extending applicability to arbitrary models.\n",
    "\n",
    "- **Model specification matters:** Maximum Likelihood Estimation performance depends critically on correct model specification. Misspecified models lead to biased and inconsistent estimators."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
