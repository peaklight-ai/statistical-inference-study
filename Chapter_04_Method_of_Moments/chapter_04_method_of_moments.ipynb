{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Method of Moments and Other Estimation Methods\n",
    "\n",
    "**Core Goal:** Estimate parameters by matching sample moments to population moments.\n",
    "\n",
    "**Motivation:** Maximum Likelihood Estimation is powerful but sometimes requires complex optimization and strong distributional assumptions. Method of Moments provides a simpler alternative: equate sample moments (mean, variance, etc.) to their population counterparts and solve for parameters. This approach often yields closed-form estimators that are easy to compute and reasonably effective. While generally less efficient than Maximum Likelihood Estimation, Method of Moments serves as a quick alternative and provides starting values for numerical Maximum Likelihood optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Moments of a Distribution\n",
    "\n",
    "**k-th Moment:** $\\mu_k = E[X^k] = \\int x^k f(x)dx$ for continuous random variable\n",
    "\n",
    "**k-th Central Moment:** $\\mu_k' = E[(X - \\mu)^k]$ where $\\mu = E[X]$\n",
    "\n",
    "**Common moments:**\n",
    "- **First moment:** $\\mu_1 = E[X]$ (mean)\n",
    "- **Second moment:** $\\mu_2 = E[X^2]$\n",
    "- **Second central moment:** $\\mu_2' = E[(X-\\mu)^2] = \\sigma^2$ (variance)\n",
    "\n",
    "**Motivation:** Moments summarize key distributional characteristics. The first few moments often uniquely determine a distribution's parameters, providing a pathway to estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal distribution N(μ=5, σ²=4)\n",
    "dist = stats.norm(5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E[X] = μ: First moment equals mean\n",
    "first_moment = dist.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E[X²] = μ² + σ²: Second moment for normal distribution\n",
    "second_moment = dist.mean()**2 + dist.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"First moment E[X] = {first_moment}\")\n",
    "print(f\"Second moment E[X²] = {second_moment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Sample Moments\n",
    "\n",
    "**k-th Sample Moment:** $m_k = \\frac{1}{n}\\sum_{i=1}^n X_i^k$\n",
    "\n",
    "**Properties:**\n",
    "- $m_1 = \\bar{X}$ (sample mean)\n",
    "- $m_2 = \\frac{1}{n}\\sum_{i=1}^n X_i^2$\n",
    "- By Law of Large Numbers: $m_k \\xrightarrow{P} \\mu_k$ as $n \\to \\infty$\n",
    "\n",
    "**Motivation:** Sample moments consistently estimate population moments. This convergence provides the foundation for Method of Moments estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "data = stats.norm(5, 2).rvs(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m₁ = (1/n)ΣXᵢ: First sample moment (sample mean)\n",
    "m1 = np.mean(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m₂ = (1/n)ΣXᵢ²: Second sample moment\n",
    "m2 = np.mean(data**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"First sample moment m₁ = {m1:.3f} (true E[X] = 5)\")\n",
    "print(f\"Second sample moment m₂ = {m2:.3f} (true E[X²] = 29)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Method of Moments Estimation\n",
    "\n",
    "**Method of Moments Principle:** Equate sample moments to population moments and solve for parameters.\n",
    "\n",
    "**For k parameters:** Set $m_j = \\mu_j(\\theta_1, ..., \\theta_k)$ for $j = 1, ..., k$ and solve the system of k equations.\n",
    "\n",
    "**Motivation:** If population moments are functions of parameters, matching sample moments to population moments provides k equations in k unknowns. Solving this system yields Method of Moments estimators. This approach is intuitive, often yields closed-form solutions, and requires minimal distributional assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Normal Distribution\n",
    "\n",
    "**Population moments:** $E[X] = \\mu$, $E[X^2] = \\mu^2 + \\sigma^2$\n",
    "\n",
    "**Method of Moments equations:**\n",
    "1. $m_1 = \\mu$\n",
    "2. $m_2 = \\mu^2 + \\sigma^2$\n",
    "\n",
    "**Solutions:**\n",
    "- $\\hat{\\mu}_{MM} = m_1 = \\bar{X}$\n",
    "- $\\hat{\\sigma}^2_{MM} = m_2 - m_1^2 = \\frac{1}{n}\\sum(X_i - \\bar{X})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# μ̂ₘₘ = m₁: Method of Moments estimator for mean\n",
    "mu_mm = m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# σ̂²ₘₘ = m₂ - m₁²: Method of Moments estimator for variance\n",
    "sigma_sq_mm = m2 - m1**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Method of Moments estimates: μ̂ = {mu_mm:.3f}, σ̂² = {sigma_sq_mm:.3f}\")\n",
    "print(f\"True parameters: μ = 5, σ² = 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** For normal distribution, Method of Moments estimators match Maximum Likelihood Estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Exponential Distribution\n",
    "\n",
    "**Distribution:** $X \\sim \\text{Exp}(\\lambda)$ with density $f(x) = \\lambda e^{-\\lambda x}$\n",
    "\n",
    "**Population moment:** $E[X] = 1/\\lambda$\n",
    "\n",
    "**Method of Moments equation:** $m_1 = 1/\\lambda$\n",
    "\n",
    "**Method of Moments estimator:** $\\hat{\\lambda}_{MM} = 1/m_1 = 1/\\bar{X}$\n",
    "\n",
    "**Note:** Matches Maximum Likelihood Estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_lambda = 2\n",
    "exp_data = stats.expon(scale=1/true_lambda).rvs(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# λ̂ₘₘ = 1/X̄: Method of Moments estimator for exponential rate\n",
    "lambda_mm = 1 / np.mean(exp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Method of Moments estimate: λ̂ₘₘ = {lambda_mm:.3f}\")\n",
    "print(f\"True λ = {true_lambda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Gamma Distribution\n",
    "\n",
    "**Distribution:** $X \\sim \\text{Gamma}(\\alpha, \\beta)$\n",
    "\n",
    "**Population moments:** $E[X] = \\alpha/\\beta$, $\\text{Var}(X) = \\alpha/\\beta^2$\n",
    "\n",
    "**Method of Moments equations:**\n",
    "1. $m_1 = \\alpha/\\beta$\n",
    "2. $m_2 - m_1^2 = \\alpha/\\beta^2$\n",
    "\n",
    "**Method of Moments estimators:**\n",
    "- $\\hat{\\alpha}_{MM} = m_1^2 / (m_2 - m_1^2) = \\bar{X}^2 / s^2$\n",
    "- $\\hat{\\beta}_{MM} = m_1 / (m_2 - m_1^2) = \\bar{X} / s^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_alpha, true_beta = 3, 2\n",
    "gamma_data = stats.gamma(true_alpha, scale=1/true_beta).rvs(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_gamma = np.mean(gamma_data)\n",
    "var_gamma = np.var(gamma_data, ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# α̂ₘₘ = X̄²/s²: Method of Moments estimator for shape parameter\n",
    "alpha_mm = mean_gamma**2 / var_gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# β̂ₘₘ = X̄/s²: Method of Moments estimator for rate parameter\n",
    "beta_mm = mean_gamma / var_gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Method of Moments estimates: α̂ₘₘ = {alpha_mm:.3f}, β̂ₘₘ = {beta_mm:.3f}\")\n",
    "print(f\"True parameters: α = {true_alpha}, β = {true_beta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Properties of Method of Moments Estimators\n",
    "\n",
    "**Consistency:** Method of Moments estimators are consistent under mild regularity conditions.\n",
    "\n",
    "**Proof sketch:** Since $m_k \\xrightarrow{P} \\mu_k$ and parameter functions are continuous, $\\hat{\\theta}_{MM} \\xrightarrow{P} \\theta$.\n",
    "\n",
    "**Asymptotic Normality:** Method of Moments estimators are asymptotically normal.\n",
    "\n",
    "**Efficiency:** Method of Moments estimators are generally NOT asymptotically efficient (do not achieve Cramér-Rao Lower Bound).\n",
    "\n",
    "**Motivation:** Method of Moments estimators have good large-sample properties but are typically less efficient than Maximum Likelihood Estimators. They provide quick, reasonable estimates when Maximum Likelihood Estimation is difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrating Consistency\n",
    "\n",
    "**As sample size increases, Method of Moments estimator converges to true parameter.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes = [20, 50, 100, 300, 1000]\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Method of Moments estimates for increasing sample sizes\n",
    "mm_estimates = [1/np.mean(stats.expon(scale=0.5).rvs(n)) for n in sample_sizes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sample_sizes, mm_estimates, 'o-', markersize=8)\n",
    "plt.axhline(true_lambda, color='r', linestyle='--', label='True λ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Sample Size n'); plt.ylabel('λ̂ₘₘ')\n",
    "plt.title('Consistency of Method of Moments Estimator'); plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Comparing Maximum Likelihood Estimation and Method of Moments\n",
    "\n",
    "**Maximum Likelihood Estimation advantages:**\n",
    "1. Asymptotically efficient (minimum variance)\n",
    "2. Invariance property\n",
    "3. Theoretically principled\n",
    "\n",
    "**Method of Moments advantages:**\n",
    "1. Often closed-form solutions\n",
    "2. Simpler to compute\n",
    "3. Requires weaker distributional assumptions\n",
    "4. Good starting values for Maximum Likelihood numerical optimization\n",
    "\n",
    "**Motivation:** Maximum Likelihood Estimation is theoretically superior but computationally more demanding. Method of Moments provides quick, reasonable estimates and serves as a practical complement to Maximum Likelihood Estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency Comparison: Normal Variance\n",
    "\n",
    "**For normal distribution with known mean:**\n",
    "- Maximum Likelihood Estimator: $\\hat{\\sigma}^2_{MLE} = \\frac{1}{n}\\sum(X_i - \\mu)^2$\n",
    "- Method of Moments: Same as Maximum Likelihood Estimator\n",
    "\n",
    "**For normal distribution with unknown mean:**\n",
    "- Maximum Likelihood Estimator: $\\hat{\\sigma}^2_{MLE} = \\frac{1}{n}\\sum(X_i - \\bar{X})^2$ (biased)\n",
    "- Method of Moments: Same as Maximum Likelihood Estimator\n",
    "\n",
    "**In this case, both methods agree.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Bayesian Estimation: A Different Paradigm\n",
    "\n",
    "**Bayesian Approach:** Treat parameter $\\theta$ as random variable with prior distribution $p(\\theta)$.\n",
    "\n",
    "**Bayes' Theorem:** $p(\\theta | x) = \\frac{p(x | \\theta) p(\\theta)}{p(x)} \\propto p(x | \\theta) p(\\theta)$\n",
    "\n",
    "**Posterior Distribution:** $p(\\theta | x)$ combines prior beliefs with data likelihood.\n",
    "\n",
    "**Point Estimates:**\n",
    "- **Posterior mean:** $\\hat{\\theta}_{Bayes} = E[\\theta | x]$\n",
    "- **Maximum a posteriori (MAP):** $\\hat{\\theta}_{MAP} = \\arg\\max_\\theta p(\\theta | x)$\n",
    "\n",
    "**Motivation:** Bayesian estimation incorporates prior information about parameters and provides full posterior distributions rather than just point estimates. This approach is philosophically different from frequentist methods (Maximum Likelihood Estimation, Method of Moments) but often yields similar results with uninformative priors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Bayesian Estimation of Normal Mean\n",
    "\n",
    "**Model:** $X_i \\sim N(\\mu, \\sigma^2)$ with $\\sigma^2$ known\n",
    "\n",
    "**Prior:** $\\mu \\sim N(\\mu_0, \\tau^2)$\n",
    "\n",
    "**Posterior:** $\\mu | x \\sim N\\left(\\frac{\\tau^2 n \\bar{x} + \\sigma^2 \\mu_0}{\\tau^2 n + \\sigma^2}, \\frac{\\tau^2 \\sigma^2}{\\tau^2 n + \\sigma^2}\\right)$\n",
    "\n",
    "**Posterior mean:** $\\hat{\\mu}_{Bayes} = \\frac{\\tau^2 n}{\\tau^2 n + \\sigma^2}\\bar{x} + \\frac{\\sigma^2}{\\tau^2 n + \\sigma^2}\\mu_0$\n",
    "\n",
    "**Interpretation:** Weighted average of data (sample mean) and prior (prior mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from N(μ=5, σ²=4)\n",
    "data_bayes = stats.norm(5, 2).rvs(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_0, tau_sq, sigma_sq = 0, 100, 4  # Prior: N(0, 100), known variance\n",
    "n = len(data_bayes); xbar = np.mean(data_bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# μ̂ᵦₐᵧₑₛ = wX̄ + (1-w)μ₀: Weighted average of data and prior\n",
    "weight_data = (tau_sq * n) / (tau_sq * n + sigma_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_bayes = weight_data * xbar + (1 - weight_data) * mu_0\n",
    "print(f\"Bayesian estimate (posterior mean): {mu_bayes:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Maximum Likelihood Estimator (sample mean): {xbar:.3f}\")\n",
    "print(f\"Prior mean: {mu_0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** With weak prior (large $\\tau^2$), Bayesian estimate approaches Maximum Likelihood Estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Robust Estimation\n",
    "\n",
    "**Robust Estimator:** Estimator whose performance does not degrade severely under violations of assumptions or presence of outliers.\n",
    "\n",
    "**Breakdown Point:** Fraction of outliers an estimator can tolerate before giving arbitrarily bad results.\n",
    "\n",
    "**Examples:**\n",
    "- **Sample mean:** Breakdown point = 0% (single outlier can ruin it)\n",
    "- **Sample median:** Breakdown point = 50% (can tolerate up to half outliers)\n",
    "- **Trimmed mean:** Breakdown point = trim fraction\n",
    "\n",
    "**Motivation:** Real data often contain outliers or violate model assumptions. Robust estimators sacrifice some efficiency under ideal conditions for stability under violations. This tradeoff is often worthwhile in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrating Lack of Robustness: Sample Mean\n",
    "\n",
    "**Single outlier dramatically affects sample mean but not median.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = stats.norm(50, 5).rvs(20)\n",
    "contaminated_data = np.append(clean_data, 500)  # Add extreme outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X̄: Sample mean (not robust to outliers)\n",
    "mean_clean = np.mean(clean_data); mean_contaminated = np.mean(contaminated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median: Robust to outliers (50% breakdown point)\n",
    "median_clean = np.median(clean_data); median_contaminated = np.median(contaminated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Clean data - Mean: {mean_clean:.2f}, Median: {median_clean:.2f}\")\n",
    "print(f\"With outlier - Mean: {mean_contaminated:.2f}, Median: {median_contaminated:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:** Single outlier changed mean by ~20 units but median by <1 unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trimmed Mean: Compromise Between Efficiency and Robustness\n",
    "\n",
    "**Trimmed Mean:** Remove fraction of smallest and largest observations, then average.\n",
    "\n",
    "**α-trimmed mean:** Remove α fraction from each tail, average the middle $(1-2\\alpha)$ fraction.\n",
    "\n",
    "**Properties:**\n",
    "- More robust than mean (positive breakdown point)\n",
    "- More efficient than median (uses more data)\n",
    "- Compromise between robustness and efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats as sp_stats\n",
    "# 10% trimmed mean: Remove 10% from each tail\n",
    "trimmed_mean = sp_stats.trim_mean(contaminated_data, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean: {mean_contaminated:.2f}\")\n",
    "print(f\"10% Trimmed mean: {trimmed_mean:.2f}\")\n",
    "print(f\"Median: {median_contaminated:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8 M-Estimators\n",
    "\n",
    "**M-Estimator:** Generalization of Maximum Likelihood Estimation where we maximize $\\sum \\rho(X_i, \\theta)$ for some function $\\rho$.\n",
    "\n",
    "**Maximum Likelihood Estimation:** Special case with $\\rho(x, \\theta) = \\log f(x; \\theta)$\n",
    "\n",
    "**Huber M-Estimator:** Combines least squares (quadratic) for small residuals with absolute deviation (linear) for large residuals.\n",
    "\n",
    "**Motivation:** M-estimators provide a framework for robust estimation. By choosing $\\rho$ to downweight outliers, we can achieve robustness while maintaining reasonable efficiency. Huber's method is a popular compromise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.9 Bootstrap for Estimator Comparison\n",
    "\n",
    "**Bootstrap:** Resample data with replacement to estimate sampling distribution of any statistic.\n",
    "\n",
    "**Use for comparison:** Estimate variance of different estimators empirically and compare.\n",
    "\n",
    "**Motivation:** When theoretical variance formulas are unavailable or complex, bootstrap provides empirical comparison of estimator precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap to compare mean versus median variance\n",
    "B = 2000; np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_bootstrap = stats.norm(50, 10).rvs(50)\n",
    "n_boot = len(data_for_bootstrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate bootstrap samples and compute statistics\n",
    "boot_means = [np.mean(np.random.choice(data_for_bootstrap, n_boot, replace=True)) for _ in range(B)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boot_medians = [np.median(np.random.choice(data_for_bootstrap, n_boot, replace=True)) for _ in range(B)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Bootstrap SE(mean): {np.std(boot_means):.3f}\")\n",
    "print(f\"Bootstrap SE(median): {np.std(boot_medians):.3f}\")\n",
    "print(f\"Efficiency ratio: {np.var(boot_medians)/np.var(boot_means):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:** Median has ~1.57 times variance of mean for normal data, confirming theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.10 Choosing Among Estimation Methods\n",
    "\n",
    "**Maximum Likelihood Estimation when:**\n",
    "- Model is well-specified and assumptions hold\n",
    "- Efficiency is critical\n",
    "- Large sample size available\n",
    "- Computational resources available for numerical optimization\n",
    "\n",
    "**Method of Moments when:**\n",
    "- Need quick estimates\n",
    "- Starting values for Maximum Likelihood numerical optimization\n",
    "- Model specification uncertain\n",
    "- Closed-form solution desired\n",
    "\n",
    "**Robust methods when:**\n",
    "- Outliers present or suspected\n",
    "- Model assumptions questionable\n",
    "- Cost of outlier influence high\n",
    "- Willing to sacrifice efficiency for stability\n",
    "\n",
    "**Bayesian methods when:**\n",
    "- Prior information available\n",
    "- Full posterior distribution desired (not just point estimate)\n",
    "- Hierarchical modeling needed\n",
    "- Small sample with informative prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: The Estimation Toolbox\n",
    "\n",
    "1. **Maximum Likelihood Estimation:** Maximize likelihood—asymptotically efficient, principled, requires correct model\n",
    "2. **Method of Moments:** Match sample and population moments—simple, quick, less efficient\n",
    "3. **Bayesian:** Combine prior and likelihood—full posterior, incorporates prior information\n",
    "4. **Robust:** Downweight outliers—stable under violations, lower efficiency under ideal conditions\n",
    "\n",
    "**No universal winner—choose based on context, assumptions, and priorities.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- **Method of Moments is simple and intuitive:** Equate sample moments to population moments and solve. Often yields closed-form estimators that are easy to compute.\n",
    "\n",
    "- **Maximum Likelihood Estimation is asymptotically superior:** Achieves minimum variance asymptotically, while Method of Moments generally does not. But Method of Moments is computationally simpler.\n",
    "\n",
    "- **Both methods are consistent and asymptotically normal:** Good large-sample properties for both, but Maximum Likelihood Estimation has smaller asymptotic variance.\n",
    "\n",
    "- **Method of Moments provides Maximum Likelihood Estimation starting values:** When Maximum Likelihood Estimation requires numerical optimization, Method of Moments estimates give good initial guesses.\n",
    "\n",
    "- **Robustness matters in practice:** Sample mean is optimal under normality but catastrophic with outliers. Median and trimmed mean sacrifice efficiency for robustness.\n",
    "\n",
    "- **Bayesian estimation incorporates prior information:** When prior knowledge exists, Bayesian methods combine it with data. With uninformative priors, results approach Maximum Likelihood Estimation.\n",
    "\n",
    "- **No single method is always best:** Maximum Likelihood Estimation for efficiency, Method of Moments for simplicity, robust methods for contaminated data, Bayesian for incorporating priors. Context determines choice.\n",
    "\n",
    "- **Bootstrap enables empirical comparison:** When theoretical properties are hard to derive, bootstrap provides empirical variance estimates for comparing methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
