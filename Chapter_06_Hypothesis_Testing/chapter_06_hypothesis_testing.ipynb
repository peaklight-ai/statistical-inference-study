{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Hypothesis Testing Fundamentals\n",
    "\n",
    "**Core Goal:** Make principled decisions about parameter values using data and controlled error rates.\n",
    "\n",
    "**Motivation:** Often we need to decide whether data support a specific claim about a parameter. Is a new drug better than placebo? Has the mean changed from its historical value? Is one method superior to another? Hypothesis testing provides a formal framework for making such decisions while controlling the probability of incorrect conclusions. Unlike confidence intervals which estimate parameters, hypothesis tests assess specific claims and provide yes/no decisions with known error rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Null and Alternative Hypotheses\n",
    "\n",
    "**Null Hypothesis (H₀):** The claim we test, typically representing \"no effect\" or \"no difference.\"\n",
    "\n",
    "**Alternative Hypothesis (H₁ or Hₐ):** What we conclude if we reject the null hypothesis.\n",
    "\n",
    "**Types of alternative hypotheses:**\n",
    "- **Two-sided:** $H_1: \\theta \\neq \\theta_0$ (parameter differs from null value)\n",
    "- **Right-sided:** $H_1: \\theta > \\theta_0$ (parameter greater than null value)\n",
    "- **Left-sided:** $H_1: \\theta < \\theta_0$ (parameter less than null value)\n",
    "\n",
    "**Motivation:** Hypothesis tests start by assuming the null hypothesis is true, then ask whether observed data are sufficiently unlikely under this assumption to reject it. The burden of proof is on demonstrating departure from H₀. This asymmetric structure protects against claiming effects that don't exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Testing Mean\n",
    "\n",
    "**Research question:** Has the population mean changed from its historical value μ₀ = 100?\n",
    "\n",
    "**Hypotheses:**\n",
    "- H₀: μ = 100 (no change)\n",
    "- H₁: μ ≠ 100 (has changed)\n",
    "\n",
    "**Data collection:** Take random sample and compute sample mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Test Statistics and Rejection Regions\n",
    "\n",
    "**Test Statistic:** A function of the data used to make the decision.\n",
    "\n",
    "**For testing μ = μ₀ with known σ:** $Z = \\frac{\\bar{X} - \\mu_0}{\\sigma/\\sqrt{n}}$\n",
    "\n",
    "**For testing μ = μ₀ with unknown σ:** $T = \\frac{\\bar{X} - \\mu_0}{S/\\sqrt{n}}$\n",
    "\n",
    "**Rejection Region:** Values of test statistic that lead to rejecting H₀.\n",
    "\n",
    "**Motivation:** Test statistics standardize the difference between data and null hypothesis. Under H₀, they follow known distributions (standard normal, t-distribution), allowing us to determine how extreme the observed value is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "mu_0 = 100; true_mu = 105; sigma = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = stats.norm(true_mu, sigma).rvs(50)\n",
    "xbar = np.mean(data); n = len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z = (X̄ - μ₀)/(σ/√n): Test statistic under H₀ (known variance)\n",
    "z_stat = (xbar - mu_0) / (sigma / np.sqrt(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Sample mean: X̄ = {xbar:.2f}\")\n",
    "print(f\"Test statistic: Z = {z_stat:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Type I and Type II Errors\n",
    "\n",
    "**Type I Error (α):** Reject H₀ when H₀ is true (false positive)\n",
    "\n",
    "**Type II Error (β):** Fail to reject H₀ when H₀ is false (false negative)\n",
    "\n",
    "**Significance Level (α):** Maximum allowable Type I error probability, typically α = 0.05\n",
    "\n",
    "**Power (1-β):** Probability of correctly rejecting H₀ when it is false\n",
    "\n",
    "| **Reality** | **Reject H₀** | **Fail to Reject H₀** |\n",
    "|------------|-------------|---------------------|\n",
    "| **H₀ True** | Type I Error (α) | Correct |\n",
    "| **H₀ False** | Correct (Power = 1-β) | Type II Error (β) |\n",
    "\n",
    "**Motivation:** All hypothesis tests involve risk of error. We cannot eliminate both types simultaneously. By convention, we control Type I error at α (usually 0.05) and try to maximize power. The asymmetry reflects that false positives (claiming effects that don't exist) are often more costly than false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 P-values\n",
    "\n",
    "**P-value:** Probability of observing a test statistic at least as extreme as the one obtained, assuming H₀ is true.\n",
    "\n",
    "**For two-sided test:** $\\text{p-value} = P(|Z| \\geq |z_{obs}| \\mid H_0)$\n",
    "\n",
    "**Decision rule:** Reject H₀ if p-value < α\n",
    "\n",
    "**Interpretation:**\n",
    "- Small p-value (< α): Data are unlikely under H₀ → reject H₀\n",
    "- Large p-value (≥ α): Data are consistent with H₀ → fail to reject H₀\n",
    "\n",
    "**Motivation:** P-values quantify how surprising the data are under the null hypothesis. They provide a continuous measure of evidence against H₀, though the reject/fail-to-reject decision is binary. Smaller p-values indicate stronger evidence against H₀."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = P(|Z| ≥ |z_obs| | H₀): Two-sided p-value\n",
    "p_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"p-value = {p_value:.4f}\")\n",
    "print(f\"Decision at α=0.05: {'Reject H₀' if p_value < 0.05 else 'Fail to reject H₀'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing P-value\n",
    "\n",
    "**P-value is the area in tails beyond observed test statistic.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-4, 4, 1000)\n",
    "y = stats.norm.pdf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'b-', linewidth=2, label='Standard Normal')\n",
    "plt.axvline(z_stat, color='r', linestyle='--', label=f'Observed Z = {z_stat:.2f}')\n",
    "plt.axvline(-z_stat, color='r', linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shade p-value region\n",
    "x_right = x[x >= abs(z_stat)]\n",
    "x_left = x[x <= -abs(z_stat)]\n",
    "plt.fill_between(x_right, stats.norm.pdf(x_right), alpha=0.3, color='red', label='p-value region')\n",
    "plt.fill_between(x_left, stats.norm.pdf(x_left), alpha=0.3, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Z'); plt.ylabel('Density')\n",
    "plt.title(f'P-value = {p_value:.4f} (Shaded Area)'); plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 One-Sample t-Test\n",
    "\n",
    "**Test:** H₀: μ = μ₀ versus H₁: μ ≠ μ₀\n",
    "\n",
    "**When σ unknown:** Use t-test with test statistic $T = \\frac{\\bar{X} - \\mu_0}{S/\\sqrt{n}} \\sim t_{n-1}$ under H₀\n",
    "\n",
    "**P-value:** $P(|T| \\geq |t_{obs}| \\mid H_0)$ using $t_{n-1}$ distribution\n",
    "\n",
    "**Motivation:** When population variance is unknown (the typical case), we use sample standard deviation and t-distribution. This is the most common hypothesis test in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate variance from data\n",
    "s = np.std(data, ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T = (X̄ - μ₀)/(S/√n): t-statistic with unknown variance\n",
    "t_stat = (xbar - mu_0) / (s / np.sqrt(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p-value from t-distribution with n-1 degrees of freedom\n",
    "p_value_t = 2 * (1 - stats.t.cdf(abs(t_stat), df=n-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"t-statistic: T = {t_stat:.3f}\")\n",
    "print(f\"p-value = {p_value_t:.4f}\")\n",
    "print(f\"Decision at α=0.05: {'Reject H₀' if p_value_t < 0.05 else 'Fail to reject H₀'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using scipy.stats for t-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats.ttest_1samp: Built-in one-sample t-test\n",
    "t_result = stats.ttest_1samp(data, mu_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Scipy t-test: t = {t_result.statistic:.3f}, p = {t_result.pvalue:.4f}\")\n",
    "print(\"Matches manual calculation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 Power of a Test\n",
    "\n",
    "**Power:** Probability of rejecting H₀ when it is false: $\\text{Power} = 1 - \\beta = P(\\text{Reject } H_0 \\mid H_1 \\text{ true})$\n",
    "\n",
    "**Factors affecting power:**\n",
    "1. **Effect size:** Larger difference from H₀ → higher power\n",
    "2. **Sample size:** Larger n → higher power\n",
    "3. **Significance level:** Larger α → higher power (but more Type I errors)\n",
    "4. **Variance:** Smaller σ² → higher power\n",
    "\n",
    "**Motivation:** Power quantifies our ability to detect effects when they exist. Low-power studies waste resources by being unlikely to detect true effects. Power analysis helps plan adequate sample sizes before data collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power Calculation Example\n",
    "\n",
    "**Question:** What is power to detect μ = 105 when testing H₀: μ = 100 with n=50, σ=15, α=0.05?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rejection region: |Z| > z_{α/2}\n",
    "alpha = 0.05; z_crit = stats.norm.ppf(1 - alpha/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Under H₁: Z ~ N(δ√n/σ, 1) where δ = μ₁ - μ₀\n",
    "delta = true_mu - mu_0; non_centrality = delta * np.sqrt(n) / sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power = P(|Z| > z_{α/2} | H₁): Probability of rejection under alternative\n",
    "power = 1 - stats.norm.cdf(z_crit - non_centrality) + stats.norm.cdf(-z_crit - non_centrality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Power to detect μ = {true_mu}: {power:.3f}\")\n",
    "print(f\"Probability of Type II error (β): {1-power:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power as Function of Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes = np.arange(10, 200, 5)\n",
    "powers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ns in sample_sizes:\n",
    "    nc = delta * np.sqrt(ns) / sigma\n",
    "    pow = 1 - stats.norm.cdf(z_crit - nc) + stats.norm.cdf(-z_crit - nc)\n",
    "    powers.append(pow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sample_sizes, powers, linewidth=2)\n",
    "plt.axhline(0.8, color='r', linestyle='--', label='80% power (conventional)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Sample Size n'); plt.ylabel('Power')\n",
    "plt.title(f'Power to Detect μ = {true_mu} (α = {alpha})'); plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7 Relationship Between Tests and Confidence Intervals\n",
    "\n",
    "**Duality:** Hypothesis tests and confidence intervals are complementary.\n",
    "\n",
    "**Connection:** Fail to reject H₀: μ = μ₀ at level α if and only if μ₀ is in the (1-α) confidence interval.\n",
    "\n",
    "**Example:**\n",
    "- 95% Confidence Interval: [98, 112]\n",
    "- Test H₀: μ = 105 at α=0.05 → Fail to reject (105 is in interval)\n",
    "- Test H₀: μ = 95 at α=0.05 → Reject (95 is not in interval)\n",
    "\n",
    "**Motivation:** This duality shows that tests and confidence intervals provide equivalent information. Confidence intervals are often more informative because they show all plausible values, not just yes/no for a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 95% confidence interval\n",
    "ci = stats.t.interval(0.95, df=n-1, loc=xbar, scale=s/np.sqrt(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"95% Confidence Interval: [{ci[0]:.2f}, {ci[1]:.2f}]\")\n",
    "print(f\"Test H₀: μ = {mu_0} → {'Reject' if mu_0 < ci[0] or mu_0 > ci[1] else 'Fail to reject'}\")\n",
    "print(f\"μ₀ = {mu_0} is {'NOT in' if mu_0 < ci[0] or mu_0 > ci[1] else 'in'} confidence interval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.8 One-Sided Tests\n",
    "\n",
    "**Right-sided test:** H₀: μ ≤ μ₀ versus H₁: μ > μ₀\n",
    "\n",
    "**Rejection region:** Z > z_α (or T > t_{α,n-1})\n",
    "\n",
    "**P-value:** P(Z ≥ z_obs | H₀)\n",
    "\n",
    "**Left-sided test:** H₀: μ ≥ μ₀ versus H₁: μ < μ₀\n",
    "\n",
    "**Rejection region:** Z < -z_α (or T < -t_{α,n-1})\n",
    "\n",
    "**P-value:** P(Z ≤ z_obs | H₀)\n",
    "\n",
    "**Motivation:** One-sided tests are appropriate when deviation in only one direction is of interest. They have higher power than two-sided tests for detecting effects in the specified direction, but cannot detect effects in the opposite direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right-sided test: H₀: μ ≤ 100 versus H₁: μ > 100\n",
    "p_value_right = 1 - stats.t.cdf(t_stat, df=n-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Right-sided p-value: {p_value_right:.4f}\")\n",
    "print(f\"Two-sided p-value: {p_value_t:.4f}\")\n",
    "print(\"One-sided p-value is half of two-sided (when test statistic positive)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.9 Common Misinterpretations of P-values\n",
    "\n",
    "**P-value is NOT:**\n",
    "1. ❌ Probability that H₀ is true\n",
    "2. ❌ Probability that results occurred by chance\n",
    "3. ❌ Importance or size of effect\n",
    "4. ❌ Probability of making an error\n",
    "\n",
    "**P-value IS:**\n",
    "✅ Probability of observing data at least as extreme as what we got, **assuming H₀ is true**\n",
    "\n",
    "**Correct statement:** \"If H₀ were true and we repeated this experiment many times, we would observe a test statistic this extreme or more in 1.2% of experiments.\"\n",
    "\n",
    "**Motivation:** P-values are commonly misunderstood, leading to incorrect conclusions. They tell us about compatibility of data with H₀, not about the truth of H₀."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.10 Practical Versus Statistical Significance\n",
    "\n",
    "**Statistical Significance:** p-value < α (typically 0.05)\n",
    "\n",
    "**Practical Significance:** Effect size large enough to matter in practice\n",
    "\n",
    "**Possible scenarios:**\n",
    "1. Statistically significant AND practically significant → Important finding\n",
    "2. Statistically significant but NOT practically significant → Trivial effect detected with large n\n",
    "3. NOT statistically significant but practically significant → Important effect missed due to small n (low power)\n",
    "4. NOT statistically significant and NOT practically significant → No evidence of meaningful effect\n",
    "\n",
    "**Motivation:** With large sample sizes, even tiny, meaningless effects can be statistically significant. Always consider effect size alongside p-value. Statistical significance is necessary but not sufficient for practical importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Hypothesis Testing Framework\n",
    "\n",
    "1. **State hypotheses:** H₀ (null) and H₁ (alternative)\n",
    "2. **Choose significance level:** α (typically 0.05)\n",
    "3. **Compute test statistic:** Standardized measure of departure from H₀\n",
    "4. **Calculate p-value:** Probability of data at least this extreme under H₀\n",
    "5. **Make decision:** Reject H₀ if p-value < α; otherwise fail to reject\n",
    "6. **Interpret:** Consider statistical significance, effect size, and practical importance\n",
    "\n",
    "**Remember:** Failing to reject H₀ does not prove H₀ is true—it means insufficient evidence against it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- **Hypothesis tests assess specific claims:** Unlike confidence intervals which estimate parameters, tests answer yes/no questions about parameter values.\n",
    "\n",
    "- **Null hypothesis gets benefit of doubt:** We assume H₀ is true and require strong evidence (small p-value) to reject it. Asymmetry favors avoiding false positives.\n",
    "\n",
    "- **Two types of errors are unavoidable:** Type I (false positive) and Type II (false negative). We control Type I error at α and try to maximize power (minimize Type II error).\n",
    "\n",
    "- **P-values measure compatibility with H₀:** Small p-value means data are unlikely under H₀, providing evidence against it. Large p-value means data are consistent with H₀.\n",
    "\n",
    "- **Statistical significance ≠ practical importance:** With large n, tiny meaningless effects can be statistically significant. Always consider effect size.\n",
    "\n",
    "- **Power depends on effect size and sample size:** Larger effects and larger samples yield higher power to detect departures from H₀.\n",
    "\n",
    "- **Tests and confidence intervals are dual:** Failing to reject H₀: μ = μ₀ is equivalent to μ₀ being in the confidence interval.\n",
    "\n",
    "- **Failure to reject ≠ proof of H₀:** Absence of evidence is not evidence of absence. May simply lack power to detect effect."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
